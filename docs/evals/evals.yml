# Groups: Coding Assistants Cybersecurity Safeguards Mathematics Reasoning Knowledge

- title: "HumanEval: Evaluating Large Language Models Trained on Code"
  description: |
    Evaluating correctness for synthesizing Python programs from docstrings. Demonstrates custom scorers and sandboxing untrusted model code.
  path: src/inspect_evals/humaneval
  arxiv: https://arxiv.org/abs/2107.03374
  group: Coding
  contributors: ["adil-a"]
  tasks: ["humaneval"]

- title: "MBPP: Mostly Basic Python Problems"
  description: |
    Measuring the ability of these models to synthesize short Python programs from natural language descriptions. Demonstrates custom scorers and sandboxing untrusted model code.
  path: src/inspect_evals/mbpp
  arxiv: https://arxiv.org/abs/2108.07732
  group: Coding
  contributors: ["jddantes"]
  tasks: ["mbpp"]

- title: "SWE-Bench: Resolving Real-World GitHub Issues"
  description: |
    Software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Demonstrates sandboxing untrusted model code.
  path: src/inspect_evals/swe_bench
  arxiv: https://arxiv.org/abs/2310.06770
  group: Coding
  contributors: ["max-kaufmann"]
  tasks: ["swe_bench"]
  dependency: "swe_bench"
  tags: ["Agent"]

- title: "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"
  description: DS-1000 is a code generation benchmark with a thousand data science problems spanning seven Python libraries.
  path: src/inspect_evals/ds1000
  arxiv: https://arxiv.org/abs/2211.11501
  group: Coding
  contributors: ["bienehito"]
  tasks: ["ds1000"]

- title: "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions"
  description: |
    Python coding benchmark with 1,140 diverse questions drawing on numerous python libraries.
  path: src/inspect_evals/bigcodebench
  arxiv: https://arxiv.org/abs/2406.15877
  group: Coding
  contributors: ["tim-hua-01"]
  tasks: ["bigcodebench"]

- title: "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation"
  description: |
    Evaluates LLMs on class-level code generation with 100 tasks constructed over 500 person-hours. The study shows that LLMs perform worse on class-level tasks compared to method-level tasks. GPT-4 and GPT-3.5 outperform other models, with holistic generation being the best strategy for them, while incremental generation works better for other models. The study also shows that the performance of LLMs is highly correlated with the number of tokens in the prompt.
  path: src/inspect_evals/class_eval
  arxiv: https://arxiv.org/abs/2308.01861
  group: Coding
  contributors: ["zhenningdavidliu"]
  tasks: ["class_eval"]

- title: "GAIA: A Benchmark for General AI Assistants"
  description: |
    GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs
  path: src/inspect_evals/gaia
  arxiv: https://arxiv.org/abs/2311.12983
  group: Assistants
  contributors: ["max-kaufmann"]
  tasks: ["gaia", "gaia_level1", "gaia_level2", "gaia_level3"]
  tags: ["Agent"]

- title: "AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?"
  description: |
    Tests whether AI agents can perform real-world time-consuming tasks on the web. Includes 214 realistic tasks covering a variety scenarios and domains.
  path: src/inspect_evals/assistant_bench
  arxiv: https://arxiv.org/abs/2407.15711
  group: Assistants
  contributors: ["nlpet"]
  tasks: ["assistant_bench"]
  tags: ["Agent"]

- title: "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models"
  description: |
    40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties.
  path: src/inspect_evals/cybench
  group: Cybersecurity
  contributors: ["sinman-aisi", "sam-deverett-dsit", "kola-aisi", "pgiav"]
  arxiv: https://arxiv.org/abs/2408.08926
  tasks: ["cybench"]
  tags: ["Agent"]

- title: "CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge"
  description: |
    Datasets containing 80, 500, 2000 and 10000 multiple-choice questions, designed to evaluate understanding across nine domains within cybersecurity
  path: src/inspect_evals/cybermetric
  arxiv: https://arxiv.org/abs/2402.07688
  group: Cybersecurity
  contributors: ["neilshaabi"]
  tasks:
    [
      "cybermetric_80",
      "cybermetric_500",
      "cybermetric_2000",
      "cybermetric_10000",
    ]

- title: "CyberSecEval_2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models"
  description: |
    Evaluates Large Language Models for risky capabilities in cybersecurity.
  path: src/inspect_evals/cyberseceval_2
  arxiv: https://arxiv.org/pdf/2404.13161
  group: Cybersecurity
  contributors: ["its-emile"]
  tasks: ["interpreter_abuse", "prompt_injection", "vulnerability_exploit"]

- title: "InterCode: Capture the Flag"
  description: |
    Measure expertise in coding, cryptography (i.e. binary exploitation, forensics), reverse engineering, and recognizing security vulnerabilities. Demonstrates tool use and sandboxing untrusted model code.
  path: src/inspect_evals/gdm_capabilities/intercode_ctf
  arxiv: https://arxiv.org/abs/2306.14898
  group: Cybersecurity
  contributors: ["jjallaire"]
  tasks: ["gdm_intercode_ctf"]
  tags: ["Agent"]

- title: "GDM Dangerous Capabilities: Capture the Flag"
  description: |
    CTF challenges covering web app vulnerabilities, off-the-shelf exploits, databases, Linux privilege escalation, password cracking and spraying. Demonstrates tool use and sandboxing untrusted model code.
  path: src/inspect_evals/gdm_capabilities/in_house_ctf
  arxiv: https://arxiv.org/abs/2403.13793
  group: Cybersecurity
  contributors: ["XkunW"]
  tasks: ["gdm_in_house_ctf"]
  tags: ["Agent"]

- title: "SEvenLLM: A benchmark to elicit, and improve cybersecurity incident analysis and response abilities in LLMs for Security Events."
  description: |
    Designed for analyzing cybersecurity incidents, which is comprised of two primary task categories: understanding and generation, with a further breakdown into 28 subcategories of tasks.
  path: src/inspect_evals/sevenllm
  group: Cybersecurity
  contributors: ["kingroryg"]
  arxiv: https://arxiv.org/abs/2405.03446
  tasks:
    ["sevenllm_mcq_zh", "sevenllm_mcq_en", "sevenllm_qa_zh", "sevenllm_qa_en"]
  dependency: "sevenllm"

- title: "SecQA: A Concise Question-Answering Dataset for Evaluating Large Language Models in Computer Security"
  description: |
    "Security Question Answering" dataset to assess LLMs' understanding and application of security principles.
    SecQA has “v1” and “v2” datasets of multiple-choice questions that aim to provide two levels of cybersecurity evaluation criteria. 
    The questions were generated by GPT-4 based on the "Computer Systems Security: Planning for Success" textbook and vetted by humans.
  path: src/inspect_evals/sec_qa
  group: Cybersecurity
  contributors: ["matthewreed26"]
  arxiv: https://arxiv.org/abs/2312.15838
  tasks: ["sec_qa_v1", "sec_qa_v1_5_shot", "sec_qa_v2", "sec_qa_v2_5_shot"]

- title: "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"
  description: |
    Diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment.
  path: src/inspect_evals/agentharm
  arxiv: https://arxiv.org/abs/2410.09024
  group: Safeguards
  contributors:
    ["alexandrasouly-aisi", "EricWinsorDSIT", "max-andr", "xanderdavies"]
  tasks: ["agentharm", "agentharm_benign"]
  tags: ["Agent"]

- title: "WMDP: Measuring and Reducing Malicious Use With Unlearning"
  description: |
    A dataset of 3,668 multiple-choice questions developed by a consortium of academics and technical consultants that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security.
  path: src/inspect_evals/wmdp
  arxiv: https://arxiv.org/abs/2403.03218
  group: Safeguards
  contributors: ["alexandraabbas"]
  tasks: ["wmdp_bio", "wmdp_chem", "wmdp_cyber"]

- title: "MATH: Measuring Mathematical Problem Solving"
  description: |
    Dataset of 12,500 challenging competition mathematics problems. Demonstrates fewshot prompting and custom scorers.
  path: src/inspect_evals/mathematics
  arxiv: https://arxiv.org/abs/2103.03874
  group: Mathematics
  contributors: ["xeon27"]
  tasks: ["math"]
  dependency: "math"

- title: "GSM8K: Training Verifiers to Solve Math Word Problems"
  description: |
    Dataset of 8.5K high quality linguistically diverse grade school math word problems. Demonstrates fewshot prompting.
  path: src/inspect_evals/gsm8k
  arxiv: https://arxiv.org/abs/2110.14168
  group: Mathematics
  contributors: ["jjallaire"]
  tasks: ["gsm8k"]

- title: "MathVista: Evaluating Mathematical Reasoning in Visual Contexts"
  path: src/inspect_evals/mathvista
  description: |
    Diverse mathematical and visual tasks that require fine-grained, deep visual understanding and compositional reasoning. Demonstrates multimodal inputs and custom scorers.
  arxiv: https://arxiv.org/abs/2310.02255
  group: Mathematics
  contributors: ["ShivMunagala"]
  tasks: ["mathvista"]
  tags: ["Multimodal"]

- title: "MGSM: Multilingual Grade School Math"
  description: |
    Extends the original GSM8K dataset by translating 250 of its problems into 10 typologically diverse languages.
  path: src/inspect_evals/mgsm
  arxiv: https://arxiv.org/abs/2210.03057
  group: Mathematics
  contributors: ["manifoldhiker"]
  tasks: ["mgsm"]

- title: "V*Bench: A Visual QA Benchmark with Detailed High-resolution Images"
  description: |
    V*Bench is a visual question & answer benchmark that evaluates MLLMs in their ability to process high-resolution and visually crowded images to find and focus on small details.
  path: src/inspect_evals/vstar_bench
  arxiv: https://arxiv.org/abs/2312.14135
  group: Reasoning
  tags: ["Multimodal"]
  contributors: ["bienehito"]
  tasks: ["vstar_bench_ar", "vstar_bench_srr"]

- title: "ARC: AI2 Reasoning Challenge"
  description: Dataset of natural, grade-school science multiple-choice questions (authored for human tests).
  path: src/inspect_evals/arc
  arxiv: https://arxiv.org/abs/1803.05457
  group: Reasoning
  contributors: ["jjallaire"]
  tasks: ["arc_easy", "arc_challenge"]

- title: "HellaSwag: Can a Machine Really Finish Your Sentence?"
  description: |
    Evaluting commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup.
  path: src/inspect_evals/hellaswag
  arxiv: https://arxiv.org/abs/1905.07830
  group: Reasoning
  contributors: ["jjallaire"]
  tasks: ["hellaswag"]

- title: "PIQA: Reasoning about Physical Commonsense in Natural Language"
  description: |
    Measure physical commonsense reasoning (e.g. "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick?")
  path: src/inspect_evals/piqa
  arxiv: https://arxiv.org/abs/1911.11641
  group: Reasoning
  contributors: ["seddy-aisi"]
  tasks: ["piqa"]

- title: "∞Bench: Extending Long Context Evaluation Beyond 100K Tokens"
  description: |
    LLM benchmark featuring an average data length surpassing 100K tokens. Comprises synthetic and realistic tasks spanning diverse domains in English and Chinese.
  path: src/inspect_evals/infinite_bench
  arxiv: https://arxiv.org/abs/2402.13718
  group: Reasoning
  contributors: ["celiawaggoner"]
  tasks:
    [
      "infinite_bench_code_debug",
      "infinite_bench_code_run",
      "infinite_bench_kv_retrieval",
      "infinite_bench_longbook_choice_eng",
      "infinite_bench_longdialogue_qa_eng",
      "infinite_bench_math_calc",
      "infinite_bench_math_find",
      "infinite_bench_number_string",
      "infinite_bench_passkey",
    ]

- title: "BBH: Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"
  description: |
    Suite of 23 challenging BIG-Bench tasks for which prior language model evaluations did not outperform the average human-rater.
  path: src/inspect_evals/bbh
  arxiv: https://arxiv.org/abs/2210.09261
  group: Reasoning
  contributors: ["JoschkaCBraun"]
  tasks: ["bbh"]

- title: "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"
  description: |
    Reading comprehension dataset that queries for complex, non-factoid information, and require difficult entailment-like inference to solve.
  path: src/inspect_evals/boolq
  arxiv: https://arxiv.org/abs/1905.10044
  group: Reasoning
  contributors: ["seddy-aisi"]
  tasks: ["boolq"]

- title: "DocVQA: A Dataset for VQA on Document Images"
  description: |
    DocVQA is a Visual Question Answering benchmark that consists of 50,000 questions covering 12,000+ document images. This implementation solves and scores the "validation" split.
  path: src/inspect_evals/docvqa
  arxiv: https://arxiv.org/abs/2007.00398
  group: Reasoning
  tags: ["Multimodal"]
  contributors: ["evanmiller-anthropic"]
  tasks: ["docvqa"]

- title: "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
  description: |
    Evaluates reading comprehension where models must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).
  path: src/inspect_evals/drop
  arxiv: https://arxiv.org/abs/1903.00161
  group: Reasoning
  contributors: ["xeon27"]
  tasks: ["drop"]

- title: "WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"
  description: |
    Set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations.
  path: src/inspect_evals/winogrande
  arxiv: https://arxiv.org/abs/1907.10641
  group: Reasoning
  contributors: ["xeon27"]
  tasks: ["winogrande"]

- title: "RACE-H: A benchmark for testing reading comprehension and reasoning abilities of neural models"
  description: |
    Reading comprehension tasks collected from the English exams for middle and high school Chinese students in the age range between 12 to 18.
  path: src/inspect_evals/race_h
  arxiv: https://arxiv.org/abs/1704.04683
  group: Reasoning
  contributors: ["mdrpanwar"]
  tasks: ["race_h"]

- title: "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark"
  description: |
    Multimodal questions from college exams, quizzes, and textbooks, covering six core disciplinestasks, demanding college-level subject knowledge and deliberate reasoning. Demonstrates multimodel inputs.
  path: src/inspect_evals/mmmu
  arxiv: https://arxiv.org/abs/2311.16502
  group: Reasoning
  contributors: ["shaheenahmedc"]
  tasks: ["mmmu_multiple_choice", "mmmu_open"]
  tags: ["Multimodal"]

- title: "SQuAD: A Reading Comprehension Benchmark requiring reasoning over Wikipedia articles"
  description: |
    Set of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.
  path: src/inspect_evals/squad
  arxiv: https://arxiv.org/abs/1606.05250
  group: Reasoning
  contributors: ["tknasir"]
  tasks: ["squad"]

- title: "IFEval: Instruction-Following Evaluation for Large Language Models"
  description: |
    Evaluates the ability to follow a set of "verifiable instructions" such as "write in more than 400 words" and "mention the keyword of AI at least 3 times. Demonstrates custom scoring.
  path: src/inspect_evals/ifeval
  arxiv: https://arxiv.org/abs/2311.07911
  group: Reasoning
  contributors: ["adil-a"]
  tasks: ["ifeval"]

- title: "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning"
  description: |
    Evaluating models on multistep soft reasoning tasks in the form of free text narratives.
  path: src/inspect_evals/musr
  arxiv: https://arxiv.org/abs/2310.16049
  group: Reasoning
  contributors: ["farrelmahaztra"]
  tasks: ["musr"]

- title: "Needle in a Haystack (NIAH): In-Context Retrieval Benchmark for Long Context LLMs"
  description: |
    NIAH evaluates in-context retrieval ability of long context LLMs by testing a model's ability to extract factual information from long-context inputs.
  path: src/inspect_evals/niah
  arxiv: https://arxiv.org/abs/2407.01437
  group: Reasoning
  tasks: ["niah"]

- title: "PAWS: Paraphrase Adversaries from Word Scrambling"
  description: |
    Evaluating models on the task of paraphrase detection by providing pairs of sentences that are either paraphrases or not.
  path: src/inspect_evals/paws
  arxiv: https://arxiv.org/abs/1904.01130
  group: Reasoning
  contributors: ["meltemkenis"]
  tasks: ["paws"]

- title: "MMLU: Measuring Massive Multitask Language Understanding"
  description: |
    Evaluate models on 57 tasks including elementary mathematics, US history, computer science, law, and more.
  path: src/inspect_evals/mmlu
  arxiv: https://arxiv.org/abs/2009.03300
  group: Knowledge
  contributors: ["jjallaire", "domdomegg"]
  tasks: ["mmlu_0_shot", "mmlu_5_shot"]

- title: "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark"
  description: |
    An enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options.
  path: src/inspect_evals/mmlu_pro
  arxiv: https://arxiv.org/abs/2406.01574
  group: Knowledge
  contributors: ["xeon27"]
  tasks: ["mmlu_pro"]

- title: "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
  description: |
    Challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry (experts at PhD level in the corresponding domains reach 65% accuracy).
  path: src/inspect_evals/gpqa
  arxiv: https://arxiv.org/abs/2311.12022
  group: Knowledge
  contributors: ["jjallaire"]
  tasks: ["gpqa_diamond"]

- title: "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
  description: |
    Measure question answering with commonsense prior knowledge.
  path: src/inspect_evals/commonsense_qa
  arxiv: https://arxiv.org/abs/1811.00937
  group: Knowledge
  contributors: ["jjallaire"]
  tasks: ["commonsense_qa"]

- title: "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
  description: |
    Measure whether a language model is truthful in generating answers to questions using questions that some humans would answer falsely due to a false belief or misconception.
  path: src/inspect_evals/truthfulqa
  arxiv: https://arxiv.org/abs/2109.07958v2
  group: Knowledge
  contributors: ["seddy-aisi"]
  tasks: ["truthfulqa"]

- title: "XSTest: A benchmark for identifying exaggerated safety behaviours in LLM's"
  description: |
    Dataset with 250 safe prompts across ten prompt types that well-calibrated models should not refuse, and 200 unsafe prompts as contrasts that models, for most applications, should refuse.
  path: src/inspect_evals/xstest
  arxiv: https://arxiv.org/abs/2308.01263
  group: Knowledge
  contributors: ["NelsonG-C"]
  tasks: ["xstest"]

- title: "PubMedQA: A Dataset for Biomedical Research Question Answering"
  description: |
    Novel biomedical question answering (QA) dataset collected from PubMed abstracts.
  path: src/inspect_evals/pubmedqa
  arxiv: https://arxiv.org/abs/1909.06146
  group: Knowledge
  contributors: ["MattFisher"]
  tasks: ["pubmedqa"]

- title: "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"
  description: |
    AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving.
  path: src/inspect_evals/agieval
  arxiv: https://arxiv.org/abs/2304.06364
  group: Knowledge
  tasks:
    [
      "agie_aqua_rat",
      "agie_logiqa_en",
      "agie_lsat_ar",
      "agie_lsat_lr",
      "agie_lsat_rc",
      "agie_math",
      "agie_sat_en",
      "agie_sat_en_without_passage",
      "agie_sat_math",
    ]
