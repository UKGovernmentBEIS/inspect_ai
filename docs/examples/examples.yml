# Groups: Coding Agents  Math Reasoning Knowledge

- title: "HumanEval: Evaluating Large Language Models Trained on Code"
  description: |
    Evaluating correctness for synthesizing Python programs from docstrings. Demonstrates custom scorers and sandboxing untrusted model code.
  path: src/inspect_evals/humaneval
  arxiv: https://arxiv.org/abs/2107.03374
  cite: chen2021evaluatinglargelanguagemodels
  group: Coding
  demonstrates: ["Scoring", "Sandbox"]
  contributors: ["adil-a"]

- title: "MBPP: Mostly Basic Python Problems"
  description: |
    Measuring the ability of these models to synthesize short Python programs from natural language descriptions. Demonstrates custom scorers and sandboxing untrusted model code.
  path: src/inspect_evals/mbpp
  arxiv: https://arxiv.org/abs/2108.07732
  cite: Kub_t_2021
  group: Coding
  demonstrates: ["Scoring", "Sandbox"]
  contributors: ["jddantes"]

- title: "SWE-Bench: Resolving Real-World GitHub Issues"
  description: |
    Software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.
    Demonstrates sandboxing untrusted model code.
  path: src/inspect_evals/swe_bench
  arxiv: https://arxiv.org/abs/2310.06770
  cite: jimenez2024swebenchlanguagemodelsresolve
  group: Coding
  demonstrates: ["Scoring", "Sandbox", "Tools"]
  contributors: ["max-kaufmann"]

- title: "GAIA: A Benchmark for General AI Assistants"
  description: |
    GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs
  path: src/inspect_evals/gaia
  arxiv: https://arxiv.org/abs/2311.12983
  cite: mialon2023gaiabenchmarkgeneralai
  group: Assistants
  demonstrates: ["Web Search", "Sandbox", "Tools"]
  contributors: ["max-kaufmann"]

- title: "InterCode: Capture the Flag"
  description: |
    Measure expertise in coding, cryptography (i.e. binary exploitation, forensics), reverse engineering, and recognizing security vulnerabilities. Demonstrates tool use and sandboxing untrusted model code.
  path: src/inspect_evals/gdm_capabilities/intercode_ctf
  arxiv: https://arxiv.org/abs/2306.14898
  cite: yang2023intercodestandardizingbenchmarkinginteractive
  group: Cybersecurity
  demonstrates: ["Scoring", "Sandbox", "Tools"]
  contributors: ["jjallaire"]

- title: "GDM Dangerous Capabilities: Capture the Flag"
  description: |
    CTF challenges covering web app vulnerabilities, off-the-shelf exploits, databases, Linux privilege escalation, password cracking and spraying. Demonstrates tool use and sandboxing untrusted model code.
  path: src/inspect_evals/gdm_capabilities/in_house_ctf
  arxiv: https://arxiv.org/abs/2403.13793
  cite: phuong2024evaluatingfrontiermodelsdangerous
  group: Cybersecurity
  demonstrates: ["Scoring", "Sandbox", "Tools"]
  contributors: ["XkunW"]

- title: "MATH: Measuring Mathematical Problem Solving"
  description: |
    Dataset of 12,500 challenging competition mathematics problems. Demonstrates fewshot prompting and custom scorers.
  path: src/inspect_evals/mathematics
  arxiv: https://arxiv.org/abs/2103.03874
  cite: hendrycks2021measuringmathematicalproblemsolving
  group: Mathematics
  demonstrates: ["Fewshot", "Scoring"]
  contributors: ["xeon27"]

- title: "GSM8K: Training Verifiers to Solve Math Word Problems"
  description: |
    Dataset of 8.5K high quality linguistically diverse grade school math word problems. Demostrates fewshot prompting.
  path: src/inspect_evals/gsm8k
  arxiv: https://arxiv.org/abs/2110.14168
  cite: cobbe2021trainingverifierssolvemath
  group: Mathematics
  demonstrates: ["Fewshot"]
  contributors: ["jjallaire"]

- title: "MathVista: Evaluating Mathematical Reasoning in Visual Contexts"
  path: src/inspect_evals/mathvista
  description: |
    Diverse mathematical and visual tasks that require fine-grained, deep visual understanding and compositional reasoning. Demonstrates multimodal inputs and custom scorers.
  arxiv: https://arxiv.org/abs/2310.02255
  cite: lu2024mathvistaevaluatingmathematicalreasoning
  group: Mathematics
  demonstrates: ["Multimodal", "Scoring"]
  contributors: ["ShivMunagala"]

- title: "ARC: AI2 Reasoning Challenge"
  description: Dataset of natural, grade-school science multiple-choice questions (authored for human tests).
  path: src/inspect_evals/arc
  arxiv: https://arxiv.org/abs/1803.05457
  cite: clark2018thinksolvedquestionanswering
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "HellaSwag: Can a Machine Really Finish Your Sentence?"
  description: |
    Evaluting commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup.
  path: evals/hellaswag
  arxiv: https://arxiv.org/abs/1905.07830
  cite: zellers2019hellaswagmachinereallyfinish
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "PIQA: Reasoning about Physical Commonsense in Natural Language"
  description: |
    Measure physical commonsense reasoning (e.g. "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick?")
  path: src/inspect_evals/piqa
  arxiv: https://arxiv.org/abs/1911.11641
  cite: bisk2019piqareasoningphysicalcommonsense
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["seddy-aisi"]

- title: "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"
  description: |
    Reading comprehension dataset that queries for complex, non-factoid information, and require difficult entailment-like inference to solve.
  path: src/inspect_evals/boolq
  arxiv: https://arxiv.org/abs/1905.10044
  cite: clark2019boolqexploringsurprisingdifficulty
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["seddy-aisi"]

- title: "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
  description: |
    Evaluates reading comprehension where models must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).
  path: src/inspect_evals/drop
  arxiv: https://arxiv.org/abs/1903.00161
  cite: dua2019dropreadingcomprehensionbenchmark
  group: Reasoning
  demonstrates: ["Fewshot"]
  contributors: ["xeon27"]

- title: "WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"
  description: |
    Set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations.
  path: src/inspect_evals/winogrande
  arxiv: https://arxiv.org/abs/1907.10641
  cite: sakaguchi2019winograndeadversarialwinogradschema
  group: Reasoning
  demonstrates: ["Fewshot", "Multiple Choice"]
  contributors: ["xeon27"]

- title: "RACE-H: A benchmark for testing reading comprehension and reasoning abilities of neural models"
  description: |
    Reading comprehension tasks collected from the English exams for middle and high school Chinese students in the age range between 12 to 18.
  path: src/inspect_evals/race_h
  arxiv: https://arxiv.org/abs/1704.04683
  cite: lai2017racelargescalereadingcomprehension
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["mdrpanwar"]

- title: "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark"
  description: |
    Multimodal questions from college exams, quizzes, and textbooks, covering six core disciplinestasks, demanding college-level subject knowledge and deliberate reasoning. Demonstrates multimodel inputs.
  path: src/inspect_evals/mmmu
  arxiv: https://arxiv.org/abs/2311.16502
  cite: yue2024mmmumassivemultidisciplinemultimodal
  group: Reasoning
  demonstrates: ["Multimodal", "Multiple Choice"]
  contributors: ["shaheenahmedc"]

- title: "SQuAD: A Reading Comprehension Benchmark requiring reasoning over Wikipedia articles"
  description: |
    Set of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.
  path: src/inspect_evals/squad
  arxiv: https://arxiv.org/abs/1606.05250
  cite: rajpurkar2016squad100000questionsmachine
  group: Reasoning
  contributors: ["tknasir"]

- title: "IFEval: Instruction-Following Evaluation for Large Language Models"
  description: |
    Evaluates the ability to follow a set of "verifiable instructions" such as "write in more than 400 words" and "mention the keyword of AI at least 3 times. Demonstrates custom scoring.
  path: src/inspect_evals/ifeval
  arxiv: https://arxiv.org/abs/2311.07911
  cite: zhou2023instructionfollowingevaluationlargelanguage
  group: Reasoning
  demonstrates: ["Scoring"]
  contributors: ["adil-a"]

- title: "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"
  description: |
    Questions from human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. Demonstrates custom scoring.
  path: evals/agieval
  arxiv: https://arxiv.org/abs/2304.06364
  cite: zhong2023agievalhumancentricbenchmarkevaluating
  group: Reasoning
  demonstrates: ["Fewshot", "Scoring"]
  contributors: ["bouromain"]

- title: "MMLU: Measuring Massive Multitask Language Understanding"
  description: |
    Evaluate models on 57 tasks including elementary mathematics, US history, computer science, law, and more.
  path: src/inspect_evals/mmlu
  arxiv: https://arxiv.org/abs/2009.03300
  cite: hendrycks2021measuringmassivemultitasklanguage
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark"
  description: |
    An enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options.
  path: src/inspect_evals/mmlu_pro
  arxiv: https://arxiv.org/abs/2406.01574
  cite: wang2024mmluprorobustchallengingmultitask
  group: Knowledge
  demonstrates: ["Fewshot", "Multiple Choice"]
  contributors: ["xeon27"]

- title: "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
  description: |
    Challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry (experts at PhD level in the corresponding domains reach 65% accuracy).
  path: src/inspect_evals/gpqa
  arxiv: https://arxiv.org/abs/2311.12022
  cite: rein2023gpqagraduatelevelgoogleproofqa
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
  description: |
    Measure question answering with commonsense prior knowledge.
  path: src/inspect_evals/commonsense_qa
  arxiv: https://arxiv.org/abs/1811.00937
  cite: talmor2019commonsenseqaquestionansweringchallenge
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
  description: |
    Measure whether a language model is truthful in generating answers to questions using questions that some humans would answer falsely due to a false belief or misconception.
  path: src/inspect_evals/truthfulqa
  arxiv: https://arxiv.org/abs/2109.07958v2
  cite: lin2022truthfulqameasuringmodelsmimic
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["seddy-aisi"]

- title: "XSTest: A benchmark for identifying exaggerated safety behaviours in LLM's"
  description: |
    Dataset with 250 safe prompts across ten prompt types that well-calibrated models should not refuse, and 200 unsafe prompts as contrasts that models, for most applications, should refuse.
  path: src/inspect_evals/xstest
  arxiv: https://arxiv.org/abs/2308.01263
  cite: r√∂ttger2024xstesttestsuiteidentifying
  group: Knowledge
  demonstrates: ["Model Grading"]
  contributors: ["NelsonG-C"]

- title: "PubMedQA: A Dataset for Biomedical Research Question Answering"
  description: |
    Novel biomedical question answering (QA) dataset collected from PubMed abstracts.
  path: src/inspect_evals/pubmedqa
  arxiv: https://arxiv.org/abs/1909.06146
  cite: jin2019pubmedqadatasetbiomedicalresearch
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["MattFisher"]
