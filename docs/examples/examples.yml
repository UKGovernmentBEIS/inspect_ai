# Groups: Coding Agents  Math Reasoning Knowledge

- title: "HumanEval: Evaluating Large Language Models Trained on Code"
  description: |
    Evaluating correctness for synthesizing Python programs from docstrings. Demonstrates custom scorers and sandboxing untrusted model code.
  path: benchmarks/humaneval
  arxiv: https://arxiv.org/abs/2107.03374
  cite: chen2021evaluatinglargelanguagemodels
  group: Coding
  demonstrates: ["Scoring", "Sandbox"]
  contributors: ["adil-a"]

- title: "MBPP: Mostly Basic Python Problems"
  description: |
    Measuring the ability of these models to synthesize short Python programs from natural language descriptions. Demonstrates custom scorers and sandboxing untrusted model code.
  path: benchmarks/mbpp
  arxiv: https://arxiv.org/abs/2108.07732
  cite: Kub_t_2021
  group: Coding
  demonstrates: ["Scoring", "Sandbox"]
  contributors: ["jddantes"]

- title: "MATH: Measuring Mathematical Problem Solving"
  description: |
    Dataset of 12,500 challenging competition mathematics problems. Demonstrates fewshot prompting and custom scorers.
  path: benchmarks/mathematics
  arxiv: https://arxiv.org/abs/2103.03874
  cite: hendrycks2021measuringmathematicalproblemsolving
  group: Math
  demonstrates: ["Fewshot", "Scoring"]
  contributors: ["xeon27"]

- title: "GSM8K: Training Verifiers to Solve Math Word Problems"
  description: |
    Dataset of 8.5K high quality linguistically diverse grade school math word problems. Demostrates fewshot prompting.
  path: benchmarks/gsm8k.py
  arxiv: https://arxiv.org/abs/2110.14168
  cite: cobbe2021trainingverifierssolvemath
  group: Math
  demonstrates: ["Fewshot"]
  contributors: ["jjallaire"]

- title: "MathVista: Evaluating Mathematical Reasoning in Visual Contexts"
  path: benchmarks/mathvista
  description: |
    Diverse mathematical and visual tasks that require fine-grained, deep visual understanding and compositional reasoning. Demonstrates multimodal inputs and custom scorers.
  arxiv: https://arxiv.org/abs/2310.02255
  cite: lu2024mathvistaevaluatingmathematicalreasoning
  group: Math
  demonstrates: ["Multimodal", "Scoring"]
  contributors: ["ShivMunagala"]

- title: "ARC: AI2 Reasoning Challenge"
  description: Dataset of natural, grade-school science multiple-choice questions (authored for human tests).
  path: benchmarks/arc.py
  arxiv: https://arxiv.org/abs/1803.05457
  cite: clark2018thinksolvedquestionanswering
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "HellaSwag: Can a Machine Really Finish Your Sentence?"
  description: |
    Evaluting commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup.
  path: benchmarks/hellaswag.py
  arxiv: https://arxiv.org/abs/1905.07830
  cite: zellers2019hellaswagmachinereallyfinish
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "PIQA: Reasoning about Physical Commonsense in Natural Language"
  description: |
    Measure physical commonsense reasoning (e.g. "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick?")
  path: benchmarks/piqa.py
  arxiv: https://arxiv.org/abs/1911.11641
  cite: bisk2019piqareasoningphysicalcommonsense
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["seddy-aisi"]

- title: "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"
  description: |
    Reading comprehension dataset that queries for complex, non-factoid information, and require difficult entailment-like inference to solve.
  path: benchmarks/boolq.py
  arxiv: https://arxiv.org/abs/1905.10044
  cite: clark2019boolqexploringsurprisingdifficulty
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["seddy-aisi"]

- title: "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
  path: benchmarks/drop
  arxiv: https://arxiv.org/abs/1903.00161
  cite: dua2019dropreadingcomprehensionbenchmark
  group: Reasoning
  demonstrates: ["Fewshot"]
  contributors: ["xeon27"]

- title: "WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"
  path: benchmarks/winogrande
  arxiv: https://arxiv.org/abs/1907.10641
  cite: sakaguchi2019winograndeadversarialwinogradschema
  group: Reasoning
  demonstrates: ["Fewshot", "Multiple Choice"]
  contributors: ["xeon27"]

- title: "RACE-H: A benchmark for testing reading comprehension and reasoning abilities of neural models"
  path: benchmarks/race-h
  arxiv: https://arxiv.org/abs/1704.04683
  cite: lai2017racelargescalereadingcomprehension
  group: Reasoning
  demonstrates: ["Multiple Choice"]
  contributors: ["mdrpanwar"]

- title: "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark"
  path: benchmarks/mmmu
  arxiv: https://arxiv.org/abs/2311.16502
  cite: yue2024mmmumassivemultidisciplinemultimodal
  group: Reasoning
  demonstrates: ["Multimodal", "Multiple Choice"]
  contributors: ["shaheenahmedc"]

- title: "SQuAD: A Reading Comprehension Benchmark requiring reasoning over Wikipedia articles"
  path: benchmarks/squad
  arxiv: https://arxiv.org/abs/1606.05250
  cite: rajpurkar2016squad100000questionsmachine
  group: Reasoning
  contributors: ["tknasir"]

- title: "IFEval: Instruction-Following Evaluation for Large Language Models"
  path: benchmarks/ifeval
  arxiv: https://arxiv.org/abs/2311.07911
  cite: zhou2023instructionfollowingevaluationlargelanguage
  group: Reasoning
  demonstrates: ["Scoring"]
  contributors: ["adil-a"]

- title: "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"
  path: benchmarks/agieval
  arxiv: https://arxiv.org/abs/2304.06364
  cite: zhong2023agievalhumancentricbenchmarkevaluating
  group: Reasoning
  demonstrates: ["Fewshot", "Scoring"]
  contributors: ["bouromain"]

- title: "MMLU: Measuring Massive Multitask Language Understanding"
  path: benchmarks/mmlu.py
  arxiv: https://arxiv.org/abs/2009.03300
  cite: hendrycks2021measuringmassivemultitasklanguage
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark"
  path: benchmarks/mmlu_pro
  arxiv: https://arxiv.org/abs/2406.01574
  cite: wang2024mmluprorobustchallengingmultitask
  group: Knowledge
  demonstrates: ["Fewshot", "Multiple Choice"]
  contributors: ["xeon27"]

- title: "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
  path: benchmarks/gpqa.py
  arxiv: https://arxiv.org/abs/2311.12022
  cite: rein2023gpqagraduatelevelgoogleproofqa
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
  path: benchmarks/commonsense_qa
  arxiv: https://arxiv.org/abs/1811.00937
  cite: talmor2019commonsenseqaquestionansweringchallenge
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["jjallaire"]

- title: "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
  path: benchmarks/truthfulqa.py
  arxiv: https://arxiv.org/abs/2109.07958v2
  cite: lin2022truthfulqameasuringmodelsmimic
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["seddy-aisi"]

- title: "XSTest: A benchmark for identifying exaggerated safety behaviours in LLM's"
  path: benchmarks/xstest
  arxiv: https://arxiv.org/abs/2308.01263
  cite: r√∂ttger2024xstesttestsuiteidentifying
  group: Knowledge
  demonstrates: ["Model Grading"]
  contributors: ["NelsonG-C"]

- title: "PubMedQA: A Dataset for Biomedical Research Question Answering"
  path: bookmarks/pubmedqa
  arxiv: https://arxiv.org/abs/1909.06146
  cite: jin2019pubmedqadatasetbiomedicalresearch
  group: Knowledge
  demonstrates: ["Multiple Choice"]
  contributors: ["MattFisher"]
