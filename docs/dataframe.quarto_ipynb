{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Log Dataframes\n",
        "---\n",
        "\n",
        "## Overview {#overview}\n",
        "\n",
        "```{=html}\n",
        "<style type=\"text/css\">\n",
        "table a {\n",
        "    white-space: nowrap;\n",
        "}\n",
        "#overview table a {\n",
        "    text-decoration: none;\n",
        "    font-family: monospace;\n",
        "    font-size: 0.95rem;\n",
        "}\n",
        "</style>\n",
        "```\n",
        "\n",
        "Inspect eval logs have a hierarchical structure which is well suited to flexibly capturing all the elements of an evaluation. However, when analysing or visualising log data you will often want to transform logs into a [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). The **inspect_ai.analysis** module includes a variety of functions for extracting [Pandas](https://pandas.pydata.org/) dataframes from logs, including:\n",
        "\n",
        "+----------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Function                   | Description                                                                                                                   |\n",
        "+============================+===============================================================================================================================+\n",
        "| [evals_df()](#evals)       | Evaluation level data (e.g. task, model, scores, etc.). One row per log file.                                                 |\n",
        "+----------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
        "| [samples_df()](#samples)   | Sample level data (e.g. input, metadata, scores, errors, etc.) One row per sample, where each log file contains many samples. |\n",
        "+----------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
        "| [messages_df()](#messages) | Message level data (e.g. role, content, etc.). One row per message, where each sample contains many messages.                 |\n",
        "+----------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
        "| [events_df()](#events)     | Event level data (type, timing, content, etc.). One row per event, where each sample contains many events.                    |\n",
        "+----------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
        "\n",
        "Each function extracts a default set of columns, however you can tailor column reading to work in whatever way you need for your analysis. Extracted dataframes can either be denormalized (e.g. if you want to immediately summarise or plot them) or normalised (e.g. if you are importing them into a SQL database).\n",
        "\n",
        "Below we'll walk through a few examples, then after that provide more in-depth documentation on customising how dataframes are read for various scenarios.\n",
        "\n",
        "## Basics\n",
        "\n",
        "### Reading Data\n",
        "\n",
        "Use the `evals_df()` function to read a dataframe containing a row for each log file:\n",
        "\n",
        "``` python\n",
        "# read logs from a given log directory\n",
        "from inspect_ai.analysis import evals_df\n",
        "evals_df(\"logs\")   \n",
        "```\n",
        "\n",
        "``` default\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 9 entries, 0 to 8\n",
        "Columns: 51 entries, eval_id to score_model_graded_qa_stderr\n",
        "```\n",
        "\n",
        "The default configuration for `evals_df()` reads a predefined set of columns. You can customise column reading in a variety of ways (covered below in [Column Definitions](#column-definitions)).\n",
        "\n",
        "Use the `samples_df()` function to read a dataframe with a record for each sample across a set of log files. For example, here we read all of the samples in the \"logs\" directory:\n",
        "\n",
        "``` python\n",
        "from inspect_ai.analysis import samples_df\n",
        "\n",
        "samples_df(\"logs\")\n",
        "```\n",
        "\n",
        "``` default\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 408 entries, 0 to 407\n",
        "Columns: 13 entries, sample_id to retries\n",
        "```\n",
        "\n",
        "By default, `sample_df()` reads all of the columns in the `EvalSampleSummary` data structure (12 columns), along with the `eval_id` for linking back to the parent eval log file.\n",
        "\n",
        "### Column Groups\n",
        "\n",
        "When reading dataframes, there are a number of pre-built column groups you can use to read various subsets of columns. For example:\n",
        "\n",
        "``` python\n",
        "from inspect_ai.analysis import (\n",
        "    EvalInfo, EvalModel, EvalResults, evals_df\n",
        ")\n",
        "\n",
        "evals_df(\n",
        "    logs=\"logs\", \n",
        "    columns=EvalInfo + EvalModel + EvalResults\n",
        ")\n",
        "```\n",
        "\n",
        "``` default\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 9 entries, 0 to 8\n",
        "Columns: 23 entries, eval_id to score_headline_value\n",
        "```\n",
        "\n",
        "This dataframe has 23 columns rather than the 51 we saw when using the default `evals_df()` congiruation, reflecting the explicit columns groups specified.\n",
        "\n",
        "You can also use column groups to join columns for doing analysis or plotting. For example, here we include eval level data along with each sample:\n",
        "\n",
        "``` python\n",
        "from inspect_ai.analysis import (\n",
        "    EvalInfo, EvalModel, SampleSummary, samples_df\n",
        ")\n",
        "\n",
        "samples_df(\n",
        "    logs=\"logs\", \n",
        "    columns=EvalInfo + EvalModel + SampleSummary\n",
        ")\n",
        "```\n",
        "\n",
        "``` default\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 408 entries, 0 to 407\n",
        "Columns: 27 entries, sample_id to retries\n",
        "```\n",
        "\n",
        "This dataframe has 27 columns rather than than the 13 we saw for the default `samples_df()` behavior, reflecting the additional eval level columns. You can create your own column groups and definitions to further customise reading (see [Column Definitions](#column-definitions) for details).\n",
        "\n",
        "### Filtering Logs\n",
        "\n",
        "The above examples read all of the logs within a given directory. You can also use the `list_eval_logs()` function to filter the list of logs based on arbitrary criteria as well control whether log listings are recursive.\n",
        "\n",
        "For example, here we read only log files with a `status` of \"success\":\n",
        "\n",
        "``` python\n",
        "# read only successful logs from a given log directory\n",
        "logs = list_eval_logs(\"logs\", filter=lambda log: log.status == \"success\")\n",
        "evals_df(logs)\n",
        "```\n",
        "\n",
        "Here we read only logs with the task name \"popularity\":\n",
        "\n",
        "``` python\n",
        "# read only logs with task name 'popularity'\n",
        "def task_filter(log: EvalLog) -> bool:\n",
        "    return log.eval.task == \"popularity\"\n",
        "    \n",
        "logs = list_eval_logs(\"logs\", filter=task_filter)\n",
        "evals_df(logs)\n",
        "```\n",
        "\n",
        "We can also choose to read a directory non-recursively:\n",
        "\n",
        "``` python\n",
        "# read only the logs at the top level of 'logs'\n",
        "logs = list_eval_logs(\"logs\", recursive=False)\n",
        "evals_df(logs)\n",
        "```\n",
        "\n",
        "### Parallel Reading\n",
        "\n",
        "The `samples_df()`, `messages_df()`, and `events_df()` functions can be slow to run if you are reading full samples from hundreds of logs, especially logs with larger samples (e.g. agent trajectories).\n",
        "\n",
        "One easy mitigation when using `samples_df()` is to stick with the default `SampleSummary` columns only, as these require only a very fast read of a header (the actual samples don't need to be loaded).\n",
        "\n",
        "If you need to read full samples, events, or messages and the read is taking longer than you'd like, you can enable parallel reading using the `parallel` option:\n",
        "\n",
        "``` python\n",
        "from inspect_ai.analysis import (\n",
        "    SampleMessages, SampleSummary samples_df, events_df\n",
        ")\n",
        "\n",
        "# we need to read full sample messages so we parallelize\n",
        "samples = samples_df(\n",
        "    \"logs\", \n",
        "    columns=SampleSummary + SampleMessages,\n",
        "    parallel=True \n",
        ")\n",
        "\n",
        "# events require fully loading samples so we parallelize\n",
        "events = events_df(\n",
        "    \"logs\",\n",
        "    parallel=True\n",
        ")\n",
        "```\n",
        "\n",
        "Parallel reading uses the Python `ProcessPoolExecutor` with the number of workers based on `mp.cpu_count()`. The workers are capped at 8 by default as typically beyond this disk and memory contention dominate performance. If you wish you can override this default by passing a number of workers explicitly:\n",
        "\n",
        "``` python\n",
        "events = events_df(\n",
        "    \"logs\",\n",
        "    parallel=16\n",
        ")\n",
        "```\n",
        "\n",
        "Note that the `evals_df()` function does not have a `parallel` option as it only does very inexpensive reads of log headers, so the overhead required for parallelisation would most often make the function slower to run.\n",
        "\n",
        "### Databases\n",
        "\n",
        "You can also read multiple dataframes and combine them into a relational database. Imported dataframes automatically include fields that can be used to join them (e.g. `eval_id` is in both the evals and samples tables).\n",
        "\n",
        "For example, here we read eval and sample level data from a log directory and import both tables into a DuckDb database:\n",
        "\n",
        "``` python\n",
        "import duckdb\n",
        "from inspect_ai.analysis import evals_df, samples_df\n",
        "\n",
        "con = duckdb.connect()\n",
        "con.register('evals', evals_df(\"logs\"))\n",
        "con.register('samples', samples_df(\"logs\"))\n",
        "```\n",
        "\n",
        "We can now execute a query to find all samples generated using the `google` provider:\n",
        "\n",
        "``` python\n",
        "result = con.execute(\"\"\"\n",
        "    SELECT * \n",
        "    FROM evals e\n",
        "    JOIN samples s ON e.eval_id = s.eval_id\n",
        "    WHERE e.model LIKE 'google/%'\n",
        "\"\"\").fetchdf()\n",
        "```\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "After reading data frames from log files, there will often be additional data preparation required for plotting or analysis. Some common transformations are provided as built in functions that satisfy the `Operation` protocol. To apply these transformations, use the `prepare()` function.\n",
        "\n",
        "For example, if you have used the [`inspect view bundle`](log-viewer.qmd#sec-publishing) command to publish logs to a website, you can use the `log_viewer()` operation to map log file paths to their published URLs:\n",
        "\n",
        "``` python\n",
        "from inspect_ai.analysis import (\n",
        "    evals_df, log_viewer, model_info, prepare\n",
        ")\n",
        "\n",
        "df = evals_df(\"logs\")\n",
        "df = prepare(df, [\n",
        "    model_info(),\n",
        "    log_viewer(\"eval\", {\"logs\": \"https://logs.example.com\"})\n",
        "])\n",
        "```\n",
        "\n",
        "See below for details on available data preparation functions.\n",
        "\n",
        "### model_info()\n",
        "\n",
        "Add additional model metadata to an eval data frame. For example:\n",
        "\n",
        "```python\n",
        "df = evals_df(\"logs\")\n",
        "df = prepare(df, model_info())\n",
        "```\n",
        "\n",
        "Fields added (when available) include:\n",
        "\n",
        "`model_organization_name`\n",
        ": Displayable model organization (e.g. OpenAI, Anthropic, etc.)\n",
        "\n",
        "`model_display_name`\n",
        ": Displayable model name (e.g. Gemini Flash 2.5)\n",
        "\n",
        "`model_snapshot`\n",
        ": A snapshot (version) string, if available (e.g. \"latest\" or \"20240229\")\n",
        "\n",
        "`model_release_date`\n",
        ": The model's release date\n",
        "                \n",
        "`model_knowledge_cutoff_date`\n",
        ": The model's knowledge cutoff date\n",
        "\n",
        "Inspect includes built in support for many models (based upon the `model` string in the dataframe). If you are using models for which Inspect does not include model metadata, you may include your own model metadata (see the `model_info()` reference for additional details).\n",
        "\n",
        "### task_info()\n",
        "\n",
        "Map task names to task display names (e.g. \"gpqa_diamond\" -> \"GPQA Diamond\").\n",
        "\n",
        "```python\n",
        "df = evals_df(\"logs\")\n",
        "df = prepare(df, [\n",
        "    task_info({\"gpqa_diamond\": \"GPQA Diamond\"})\n",
        "])\n",
        "```\n",
        "\n",
        "See the `task_info()` reference for additional details.\n",
        "\n",
        "### log_viewer()\n",
        "\n",
        "Add a \"log_viewer\" column to an eval data frame by mapping log file paths to remote URLs. Pass mappings from the local log directory (or S3 bucket) to the URL where the logs have been publishing using [`inspect view bundle`](https://inspect.aisi.org.uk/log-viewer.html#sec-publishing). For example:\n",
        "\n",
        "```python\n",
        "df = evals_df(\"logs\")\n",
        "df = prepare(df, [\n",
        "    log_viewer(\"eval\", {\"logs\": \"https://logs.example.com\"})\n",
        "])\n",
        "```\n",
        "\n",
        "Note that the code above targets \"eval\" (the top level viewer page for an eval). Other available targets include \"sample\", \"event\", and \"message\". See the `log_viewer()` reference for additional details.\n",
        "\n",
        "### frontier()\n",
        "\n",
        "Adds a \"frontier\" column to each task. The value of the \"frontier\" column will be `True` if for the task, the model was the top-scoring model among all models available at the moment the model was released; otherwise it will be `False`.\n",
        "\n",
        "The `frontier()` requires scores and model release dates, so must be run after the `model_info()` operation."
      ],
      "id": "4bebda64"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from inspect_ai.analysis import (\n",
        "    evals_df, frontier, log_viewer, model_info, prepare\n",
        ")\n",
        "\n",
        "df = evals_df(\"logs\")\n",
        "df = prepare(df, [\n",
        "    model_info(),\n",
        "    frontier()\n",
        "])"
      ],
      "id": "00fd8792",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### score_to_float()\n",
        "\n",
        "Converts one or more score columns to a float representation of the score. \n",
        "\n",
        "For each column specified, this operation will convert the values to floats using the provided `value_to_float` function. The column value will be replaced with the float value."
      ],
      "id": "fc494168"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from inspect_ai.analysis import (\n",
        "    samples_df, frontier, model_info, prepare, score_to_float\n",
        ")\n",
        "\n",
        "df = samples_df(\"logs\")\n",
        "df = prepare(df, [\n",
        "    score_to_float(\"score_includes\")\n",
        "])"
      ],
      "id": "4f84cfe8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Definitions {#column-definitions}\n",
        "\n",
        "The examples above all use built-in column specifications (e.g. `EvalModel`, `EvalResults`, `SampleSummary`, etc.). These specifications exist as a convenient starting point but can be replaced fully or partially by your own custom definitions.\n",
        "\n",
        "Column definitions specify how JSON data is mapped into dataframe columns, and are specified using subclasses of the `Column` class (e.g. `EvalColumn`, `SampleColumn`). For example, here is the definition of the built-in `EvalTask` column group:\n",
        "\n",
        "``` python\n",
        "EvalTask: list[Column] = [\n",
        "    EvalColumn(\"task_name\", path=\"eval.task\", required=True),\n",
        "    EvalColumn(\"task_version\", path=\"eval.task_version\", required=True),\n",
        "    EvalColumn(\"task_file\", path=\"eval.task_file\"),\n",
        "    EvalColumn(\"task_attribs\", path=\"eval.task_attribs\"),\n",
        "    EvalColumn(\"task_arg_*\", path=\"eval.task_args\"),\n",
        "    EvalColumn(\"solver\", path=\"eval.solver\"),\n",
        "    EvalColumn(\"solver_args\", path=\"eval.solver_args\"),\n",
        "    EvalColumn(\"sandbox_type\", path=\"eval.sandbox.type\"),\n",
        "    EvalColumn(\"sandbox_config\", path=\"eval.sandbox.config\"),\n",
        "]\n",
        "```\n",
        "\n",
        "Columns are defined with a `name`, a `path` (location within JSON to read their value from), and other options (e.g. `required`, `type`, etc.) . Column paths use [JSON Path](https://github.com/h2non/jsonpath-ng) expressions to indicate how they should be read from JSON.\n",
        "\n",
        "Many fields within eval logs are optional, and path expressions will automatically resolve to `None` when they include a missing field (unless the `required=True` option is specified).\n",
        "\n",
        "Here are are all of the options available for `Column` definitions:\n",
        "\n",
        "#### Column Options\n",
        "\n",
        "+------------+------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Parameter  | Type                               | Description                                                                                                                                                                                             |\n",
        "+============+====================================+=========================================================================================================================================================================================================+\n",
        "| `name`     | `str`                              | Column name for dataframe. Can include wildcard characters (e.g. `task_arg_*`) for mapping dictionaries into multiple columns.                                                                          |\n",
        "+------------+------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| `path`     | `str` \\| `JSONPath`                | Path into JSON to extract the column from (uses [JSON Path](https://github.com/h2non/jsonpath-ng) expressions). Subclasses also implement path handlers that take e.g. an `EvalLog` and return a value. |\n",
        "+------------+------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| `required` | `bool`                             | Is the field required (i.e. should an error occur if it not found).                                                                                                                                     |\n",
        "+------------+------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| `default`  | `JsonValue`                        | Default value to yield if the field or its parents are not found in JSON.                                                                                                                               |\n",
        "+------------+------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| `type`     | `Type[ColumnType]`                 | Validation check and directive to attempt to coerce the data into the specified `type`. Coercion from `str` to other types is done after interpreting the string using YAML (e.g. `\"true\"` -\\> `True`). |\n",
        "+------------+------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| `value`    | `Callable[[JsonValue], JsonValue]` | Function used to transform the value read from JSON into a value for the dataframe (e.g. converting a `list` to a comma-separated `str`).                                                               |\n",
        "+------------+------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
        "\n",
        "Here are some examples that demonstrate the use of various options:\n",
        "\n",
        "``` python\n",
        "# required field\n",
        "EvalColumn(\"run_id\", path=\"eval.run_id\", required=True)\n",
        "\n",
        "# coerce field from int to str\n",
        "SampleColumn(\"id\", path=\"id\", required=True, type=str)\n",
        "\n",
        "# split metadata dict into multiple columns\n",
        "SampleColumn(\"metadata_*\", path=\"metadata\")\n",
        "\n",
        "# transform list[str] to str\n",
        "SampleColumn(\"target\", path=\"target\", value=list_as_str),\n",
        "```\n",
        "\n",
        "#### Column Merging\n",
        "\n",
        "If a column is name is repeated within a list of columns then the column definition encountered last is utilised. This makes it straightforward to override default column definitions. For example, here we override the behaviour of the default sample `metadata` columns (keeping it as JSON rather than splitting it into multiple columns):\n",
        "\n",
        "``` python\n",
        " samples_df(\n",
        "     logs=\"logs\",\n",
        "     columns=SampleSummary + [SampleColumn(\"metadata\", path=\"metadata\")]\n",
        " )\n",
        "```\n",
        "\n",
        "#### Strict Mode\n",
        "\n",
        "By default, dataframes are read in `strict` mode, which means that if fields are missing or paths are invalid an error is raised and the import is aborted. You can optionally set `strict=False`, in which case importing will proceed and a tuple containing `pd.DataFrame` and a list of any errors encountered is returned. For example:\n",
        "\n",
        "``` python\n",
        "from inspect_ai.analysis import evals_df\n",
        "\n",
        "evals, errors = evals_df(\"logs\", strict=False)\n",
        "if len(errors) > 0:\n",
        "    print(errors)\n",
        "```\n",
        "\n",
        "### Evals {#evals}\n",
        "\n",
        "`EvalColumns` defines a default set of roughly 50 columns to read from the top level of an eval log. `EvalColumns` is in turn composed of several sets of column definitions that you can be used independently, these include:\n",
        "\n",
        "+---------------+--------------------------------------------------------------------------+\n",
        "| Type          | Description                                                              |\n",
        "+===============+==========================================================================+\n",
        "| `EvalInfo`    | Descriptive information (e.g. created, tags, metadata, git commit, etc.) |\n",
        "+---------------+--------------------------------------------------------------------------+\n",
        "| `EvalTask`    | Task configuration (name, file, args, solver, etc.)                      |\n",
        "+---------------+--------------------------------------------------------------------------+\n",
        "| `EvalModel`   | Model name, args, generation config, etc.                                |\n",
        "+---------------+--------------------------------------------------------------------------+\n",
        "| `EvalDataset` | Dataset name, location, sample ids, etc.                                 |\n",
        "+---------------+--------------------------------------------------------------------------+\n",
        "| `EvalConfig`  | Epochs, approval, sample limits, etc.                                    |\n",
        "+---------------+--------------------------------------------------------------------------+\n",
        "| `EvalResults` | Status, errors, samples completed, headline metric.                      |\n",
        "+---------------+--------------------------------------------------------------------------+\n",
        "| `EvalScores`  | All scores and metrics broken into separate columns.                     |\n",
        "+---------------+--------------------------------------------------------------------------+\n",
        "\n",
        "#### Multi-Columns\n",
        "\n",
        "The `task_args` dictionary and eval scores data structure are both expanded into multiple columns by default:\n",
        "\n",
        "``` python\n",
        "EvalColumn(\"task_arg_*\", path=\"eval.task_args\")\n",
        "EvalColumn(\"score_*_*\", path=eval_log_scores_dict)\n",
        "```\n",
        "\n",
        "Note that scores are a two-level dictionary of `score_<scorer>_<metric>` and are extracted using a custom function. If you want to handle scores a different way you can build your own set of eval columns with a custom scores handler. For example, here we take a subset of eval columns along with our own custom handler (`custom_scores_fn`) for scores:\n",
        "\n",
        "``` python\n",
        "evals_df(\n",
        "    logs=\"logs\", \n",
        "    columns=(\n",
        "        EvalInfo\n",
        "        + EvalModel\n",
        "        + EvalResults\n",
        "        + ([EvalColumn(\"score_*_*\", path=custom_scores_fn)])\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "#### Custom Extraction\n",
        "\n",
        "The example above demonstrates the use of custom extraction functions, which take an `EvalLog` and return a `JsonValue`.\n",
        "\n",
        "For example, here is the default extraction function for the the dictionary of scores/metrics:\n",
        "\n",
        "``` python\n",
        "def scores_dict(log: EvalLog) -> JsonValue:\n",
        "    if log.results is None:\n",
        "        return None\n",
        "    \n",
        "    metrics: JsonValue = [\n",
        "        {\n",
        "            score.name: {\n",
        "                metric.name: metric.value for metric in score.metrics.values()\n",
        "            }\n",
        "        }\n",
        "        for score in log.results.scores\n",
        "    ]\n",
        "    return metrics\n",
        "```\n",
        "\n",
        "Which is then used in the definition of the `EvalScores` column group as follows:\n",
        "\n",
        "``` python\n",
        "EvalScores: list[Column] = [\n",
        "    EvalColumn(\"score_*_*\", path=scores_dict),\n",
        "]\n",
        "```\n",
        "\n",
        "### Samples {#samples}\n",
        "\n",
        "The `samples_df()` function can read from either sample summaries (`EvalSampleSummary`) or full sample records (`EvalSample`).\n",
        "\n",
        "By default, the `SampleSummary` column group is used, which reads only from summaries, resulting in considerably higher performance than reading full samples.\n",
        "\n",
        "``` python\n",
        "SampleSummary: list[Column] = [\n",
        "    SampleColumn(\"id\", path=\"id\", required=True, type=str),\n",
        "    SampleColumn(\"epoch\", path=\"epoch\", required=True),\n",
        "    SampleColumn(\"input\", path=sample_input_as_str, required=True),\n",
        "    SampleColumn(\"target\", path=\"target\", required=True, value=list_as_str),\n",
        "    SampleColumn(\"metadata_*\", path=\"metadata\"),\n",
        "    SampleColumn(\"score_*\", path=\"scores\", value=score_values),\n",
        "    SampleColumn(\"model_usage\", path=\"model_usage\"),\n",
        "    SampleColumn(\"total_time\", path=\"total_time\"),\n",
        "    SampleColumn(\"working_time\", path=\"total_time\"),\n",
        "    SampleColumn(\"error\", path=\"error\"),\n",
        "    SampleColumn(\"limit\", path=\"limit\"),\n",
        "    SampleColumn(\"retries\", path=\"retries\"),\n",
        "]\n",
        "```\n",
        "\n",
        "If you want to read all of the messages contained in a sample into a string column, use the `SampleMessages` column group. For example, here we read the summary field and the messages:\n",
        "\n",
        "``` python\n",
        "from inspect_ai.analysis import (\n",
        "    SampleMessages, SampleSummary, samples_df\n",
        ")\n",
        "\n",
        "samples_df(\n",
        "    logs=\"logs\", \n",
        "    columns = SampleSummary + SampleMessages\n",
        ")\n",
        "```\n",
        "\n",
        "Note that reading `SampleMessages` requires reading full sample content, so will take considerably longer than reading only summaries.\n",
        "\n",
        "When you create a samples data frame the `eval_id` of its parent evaluation is automatically included. You can additionally include other fields from the evals table, for example:\n",
        "\n",
        "``` python\n",
        "samples_df(\n",
        "    logs=\"logs\", \n",
        "    columns = EvalModel + SampleSummary + SampleMessages\n",
        ")\n",
        "```\n",
        "\n",
        "#### Multi-Columns\n",
        "\n",
        "Note that the `metadata` and `score` columns are both dictionaries that are expanded into multiple columns:\n",
        "\n",
        "``` python\n",
        "SampleColumn(\"metadata_*\", path=\"metadata\")\n",
        "SampleColumn(\"score_*\", path=\"scores\", value=score_values)\n",
        "```\n",
        "\n",
        "This might or might not be what you want for your data frame. To preserve them as JSON, remove the `_*`:\n",
        "\n",
        "``` python\n",
        "SampleColumn(\"metadata\", path=\"metadata\")\n",
        "SampleColumn(\"score\", path=\"scores\")\n",
        "```\n",
        "\n",
        "You could also write a custom [extraction](#custom-extraction-1) handler to read them in some other way.\n",
        "\n",
        "#### Full Samples\n",
        "\n",
        "`SampleColumn` will automatically determine whether it is referencing a field that requires a full sample read (for example, `messages` or `store`). There are five fields in sample summaries that have reduced footprint in the summary (`input`, `metadata`, and `scores`, `error`, and `limit`). For these, fields specify `full=True` to force reading from the full sample record. For example:\n",
        "\n",
        "``` python\n",
        "SampleColumn(\"limit_type\", path=\"limit.type\", full=True)\n",
        "SampleColumn(\"limit_value\", path=\"limit.limit\", full=True)\n",
        "```\n",
        "\n",
        "If you are only interested in reading full values for `metadata`, you can use `full=True` when calling `samples_df()` as shorthand for this:\n",
        "\n",
        "```python\n",
        "samples_df(logs=\"logs\", full=True)\n",
        "```\n",
        "\n",
        "#### Custom Extraction {#custom-extraction-1}\n",
        "\n",
        "As with `EvalColumn`, you can also extract data from a sample using a callback function passed as the `path`:\n",
        "\n",
        "``` python\n",
        "def model_reasoning_tokens(summary: EvalSampleSummary) -> JsonValue:\n",
        "    ## extract reasoning tokens from summary.model_usage\n",
        "\n",
        "SampleColumn(\"model_reasoning_tokens\", path=model_reasoning_tokens)\n",
        "```\n",
        "\n",
        "::: {.callout-note appearance=\"simple\"}\n",
        "Sample summaries were enhanced in version 0.3.93 (May 1, 2025) to include the `metadata`, `model_usage`, `total_time`, `working_time`, and `retries` fields. If you need to read any of these values you can update older logs with the new fields by round-tripping them through `inspect log convert`. For example:\n",
        "\n",
        "``` bash\n",
        "$ inspect log convert ./logs --to eval --output-dir ./logs-amended\n",
        "```\n",
        ":::\n",
        "\n",
        "#### Sample IDs\n",
        "\n",
        "The `samples_df()` function produces a globally unique ID for each sample, contained in the `sample_id` field. This field is also included in the data frames created by `messages_df()` and `events_df()` as a parent sample reference.\n",
        "\n",
        "Since `sample_id` is globally unique, it is suitable for use in tables and views that span multiple evaluations.\n",
        "\n",
        "Note that `samples_df()` also includes `id` and `epoch` fields that serve distinct purposes: `id` references the corresponding sample in the task's dataset, while `epoch` indicates the iteration of execution.\n",
        "\n",
        "### Messages {#messages}\n",
        "\n",
        "The `messages_df()` function enables reading message level data from a set of eval logs. Each row corresponds to a message, and includes a `sample_id` and `eval_id` for linking back to its parents.\n",
        "\n",
        "The `messages_df()` function takes a `filter` parameter which can either be a list of `role` designations or a function that performs filtering. For example:\n",
        "\n",
        "``` python\n",
        "assistant_messages = messages_df(\"logs\", filter=[\"assistant\"])\n",
        "```\n",
        "\n",
        "#### Default Columns\n",
        "\n",
        "The default `MessageColumns` includes `MessageContent` and `MessageToolCalls`:\n",
        "\n",
        "``` python\n",
        "MessageContent: list[Column] = [\n",
        "    MessageColumn(\"role\", path=\"role\", required=True),\n",
        "    MessageColumn(\"content\", path=message_text),\n",
        "    MessageColumn(\"source\", path=\"source\"),\n",
        "]\n",
        "\n",
        "MessageToolCalls: list[Column] = [\n",
        "    MessageColumn(\"tool_calls\", path=message_tool_calls),\n",
        "    MessageColumn(\"tool_call_id\", path=\"tool_call_id\"),\n",
        "    MessageColumn(\"tool_call_function\", path=\"function\"),\n",
        "    MessageColumn(\"tool_call_error\", path=\"error.message\"),\n",
        "]\n",
        "\n",
        "MessageColumns: list[Column] = MessageContent + MessageToolCalls\n",
        "```\n",
        "\n",
        "When you create a messages data frame the parent `sample_id` and `eval_id` are automatically included in each record. You can additionally include other fields from these tables, for example:\n",
        "\n",
        "``` python\n",
        "messages = messages_df(\n",
        "    logs=\"logs\",\n",
        "    columns=EvalModel + MessageColumns             \n",
        ")\n",
        "```\n",
        "\n",
        "#### Custom Extraction\n",
        "\n",
        "Two of the fields above are resolved using custom extraction functions (`content` and `tool_calls`). Here is the source code for those functions:\n",
        "\n",
        "``` python\n",
        "def message_text(message: ChatMessage) -> str:\n",
        "    return message.text\n",
        "\n",
        "def message_tool_calls(message: ChatMessage) -> str | None:\n",
        "    if isinstance(message, ChatMessageAssistant) and message.tool_calls is not None:\n",
        "        tool_calls = \"\\n\".join(\n",
        "            [\n",
        "                format_function_call(\n",
        "                    tool_call.function, tool_call.arguments, width=1000\n",
        "                )\n",
        "                for tool_call in message.tool_calls\n",
        "            ]\n",
        "        )\n",
        "        return tool_calls\n",
        "    else:\n",
        "        return None\n",
        "```\n",
        "\n",
        "### Events {#events}\n",
        "\n",
        "The `events_df()` function enables reading event level data from a set of eval logs. Each row corresponds to an event, and includes a `sample_id` and `eval_id` for linking back to its parents.\n",
        "\n",
        "Because events are so heterogeneous, there is no default `columns` specification for calls to `events_df()`. Rather, you can compose columns from the following pre-built groups:\n",
        "\n",
        "+---------------------+--------------------------------------------------------+\n",
        "| Type                | Description                                            |\n",
        "+=====================+========================================================+\n",
        "| `EventInfo`         | Event type and span id.                                |\n",
        "+---------------------+--------------------------------------------------------+\n",
        "| `EventTiming`       | Start and end times (both clock time and working time) |\n",
        "+---------------------+--------------------------------------------------------+\n",
        "| `ModelEventColumns` | Read data from model events.                           |\n",
        "+---------------------+--------------------------------------------------------+\n",
        "| `ToolEventColumns`  | Read data from tool events.                            |\n",
        "+---------------------+--------------------------------------------------------+\n",
        "\n",
        "The `events_df()` function also takes a `filter` parameter which can provide a function that performs filtering. For example, to read all model events:\n",
        "\n",
        "``` python\n",
        "def model_event_filter(event: Event) -> bool:\n",
        "    return event.event == \"model\"\n",
        "\n",
        "model_events = events_df(\n",
        "    logs=\"logs\", \n",
        "    columns=EventTiming + ModelEventColumns,\n",
        "    filter=model_event_filter\n",
        ")\n",
        "```\n",
        "\n",
        "To read all tool events:\n",
        "\n",
        "``` python\n",
        "def tool_event_filter(event: Event) -> bool:\n",
        "    return event.event == \"tool\"\n",
        "\n",
        "model_events = events_df(\n",
        "    logs=\"logs\", \n",
        "    columns=EvalModel + EventTiming + ToolEventColumns,\n",
        "    filter=tool_event_filter\n",
        ")\n",
        "```\n",
        "\n",
        "Note that for tool events we also include the `EvalModel` column group as model information is not directly embedded in tool events (whereas it is within model events).\n",
        "\n",
        "### Custom\n",
        "\n",
        "You can create custom column types that extract data based on additional parameters. For example, imagine you want to write a set of extraction functions that are passed a `ReportConfig` and an `EvalLog` (the report configuration might specify scores to extract, normalisation constraints, etc.)\n",
        "\n",
        "Here we define a new `ReportColumn` class that derives from `EvalColumn`:\n",
        "\n",
        "``` python\n",
        "import functools\n",
        "from typing import Callable\n",
        "from pydantic import BaseModel, JsonValue\n",
        "\n",
        "from inspect_ai.log import EvalLog\n",
        "from inspect_ai.analysis import EvalColumn\n",
        "\n",
        "class ReportConfig(BaseModel):\n",
        "    # config fields\n",
        "    ...\n",
        "\n",
        "class ReportColumn(EvalColumn):\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        config: ReportConfig,\n",
        "        extract: Callable[[ReportConfig, EvalLog], JsonValue],\n",
        "        *,\n",
        "        required: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(\n",
        "            name=name,\n",
        "            path=functools.partial(extract, config),\n",
        "            required=required,\n",
        "        )\n",
        "```\n",
        "\n",
        "The key here is using [functools.partial](https://www.geeksforgeeks.org/partial-functions-python/) to adapt the function that takes `config` and `log` into a function that takes `log` (which is what the `EvalColumn` class works with).\n",
        "\n",
        "We can now create extraction functions that take a `ReportConfig` and an `EvalLog` and pass them to `ReportColumn`:\n",
        "\n",
        "``` python\n",
        "# read dict scores from log according to config\n",
        "def read_scores(config: ReportConfig, log: EvalLog) -> JsonValue:\n",
        "    ...\n",
        "\n",
        "# config for a given report\n",
        "config = ReportConfig(...)\n",
        "\n",
        "# column that reads scores from log based on config\n",
        "ReportColumn(\"score_*\", config, read_scores)\n",
        "```"
      ],
      "id": "16d8bf80"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/charlesteague/Development/inspect_ai/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}