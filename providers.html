<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Model Providers – Inspect</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./caching.html" rel="next">
<link href="./models.html" rel="prev">
<link href="./favicon.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-871b74c1c552ddee99e75c490274fea0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<meta property="og:title" content="Inspect">
<meta property="og:description" content="Open-source framework for large language model evaluations">
<meta property="og:image" content="https://inspect.aisi.org.uk//images/inspect.png">
<meta property="og:site_name" content="Inspect">
<meta property="og:image:height" content="1258">
<meta property="og:image:width" content="2400">
<meta name="twitter:title" content="Inspect">
<meta name="twitter:description" content="Open-source framework for large language model evaluations">
<meta name="twitter:image" content="https://inspect.aisi.org.uk//images/inspect.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="1258">
<meta name="twitter:image-width" content="2400">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./images/aisi-logo.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Inspect AI</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">User Guide</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./reference/index.html"> 
<span class="menu-text">Reference</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./extensions/index.html"> 
<span class="menu-text">Extensions</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./evals/index.html"> 
<span class="menu-text">Evals</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./CHANGELOG.html"> 
<span class="menu-text">Changelog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/UKGovernmentBEIS/inspect_ai"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./models.html">Models</a></li><li class="breadcrumb-item"><a href="./providers.html">Providers</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Basics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./options.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Options</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./log-viewer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Log Viewer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./vscode.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VS Code</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Components</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tasks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./solvers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Solvers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./scorers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scorers</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./providers.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Providers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./caching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Caching</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./models-batch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Mode</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multimodal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multimodal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reasoning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./structured.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structured Output</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tool Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tools-standard.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Standard Tools</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tools-mcp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MCP Tools</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tools-custom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Tools</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sandboxing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sandboxing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./approval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tool Approval</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Agents</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./agents.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Agents</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./react-agent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ReAct Agent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multi-agent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multi Agent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./agent-custom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Agents</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./agent-bridge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Agent Bridge</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./human-agent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Human Agent</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eval-logs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Log Files</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dataframe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Log Dataframes</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Advanced</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eval-sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Eval Sets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./errors-and-limits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Errors &amp; Limits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./typing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Typing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tracing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tracing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parallelism.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interactivity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Interactivity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extensions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Extensions</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#openai" id="toc-openai" class="nav-link" data-scroll-target="#openai">OpenAI</a>
  <ul class="collapse">
  <li><a href="#model-args" id="toc-model-args" class="nav-link" data-scroll-target="#model-args">Model Args</a></li>
  <li><a href="#responses-api" id="toc-responses-api" class="nav-link" data-scroll-target="#responses-api">Responses API</a></li>
  <li><a href="#responses-store" id="toc-responses-store" class="nav-link" data-scroll-target="#responses-store">Responses Store</a></li>
  <li><a href="#flex-processing" id="toc-flex-processing" class="nav-link" data-scroll-target="#flex-processing">Flex Processing</a></li>
  <li><a href="#openai-on-azure" id="toc-openai-on-azure" class="nav-link" data-scroll-target="#openai-on-azure">OpenAI on Azure</a></li>
  </ul></li>
  <li><a href="#anthropic" id="toc-anthropic" class="nav-link" data-scroll-target="#anthropic">Anthropic</a>
  <ul class="collapse">
  <li><a href="#betas" id="toc-betas" class="nav-link" data-scroll-target="#betas">Betas</a></li>
  <li><a href="#streaming" id="toc-streaming" class="nav-link" data-scroll-target="#streaming">Streaming</a></li>
  <li><a href="#anthropic-on-aws-bedrock" id="toc-anthropic-on-aws-bedrock" class="nav-link" data-scroll-target="#anthropic-on-aws-bedrock">Anthropic on AWS Bedrock</a></li>
  <li><a href="#anthropic-on-vertex-ai" id="toc-anthropic-on-vertex-ai" class="nav-link" data-scroll-target="#anthropic-on-vertex-ai">Anthropic on Vertex AI</a></li>
  </ul></li>
  <li><a href="#google-gemini" id="toc-google-gemini" class="nav-link" data-scroll-target="#google-gemini">Google</a>
  <ul class="collapse">
  <li><a href="#gemini-on-vertex-ai" id="toc-gemini-on-vertex-ai" class="nav-link" data-scroll-target="#gemini-on-vertex-ai">Gemini on Vertex AI</a></li>
  <li><a href="#safety-settings" id="toc-safety-settings" class="nav-link" data-scroll-target="#safety-settings">Safety Settings</a></li>
  </ul></li>
  <li><a href="#mistral" id="toc-mistral" class="nav-link" data-scroll-target="#mistral">Mistral</a>
  <ul class="collapse">
  <li><a href="#mistral-on-azure-ai" id="toc-mistral-on-azure-ai" class="nav-link" data-scroll-target="#mistral-on-azure-ai">Mistral on Azure AI</a></li>
  </ul></li>
  <li><a href="#deepseek" id="toc-deepseek" class="nav-link" data-scroll-target="#deepseek">DeepSeek</a></li>
  <li><a href="#grok" id="toc-grok" class="nav-link" data-scroll-target="#grok">Grok</a></li>
  <li><a href="#aws-bedrock" id="toc-aws-bedrock" class="nav-link" data-scroll-target="#aws-bedrock">AWS Bedrock</a></li>
  <li><a href="#azure-ai" id="toc-azure-ai" class="nav-link" data-scroll-target="#azure-ai">Azure AI</a>
  <ul class="collapse">
  <li><a href="#tool-emulation" id="toc-tool-emulation" class="nav-link" data-scroll-target="#tool-emulation">Tool Emulation</a></li>
  </ul></li>
  <li><a href="#together-ai" id="toc-together-ai" class="nav-link" data-scroll-target="#together-ai">Together AI</a></li>
  <li><a href="#groq" id="toc-groq" class="nav-link" data-scroll-target="#groq">Groq</a></li>
  <li><a href="#fireworks-ai" id="toc-fireworks-ai" class="nav-link" data-scroll-target="#fireworks-ai">Fireworks AI</a></li>
  <li><a href="#sambanova" id="toc-sambanova" class="nav-link" data-scroll-target="#sambanova">SambaNova</a></li>
  <li><a href="#cloudflare" id="toc-cloudflare" class="nav-link" data-scroll-target="#cloudflare">Cloudflare</a></li>
  <li><a href="#perplexity" id="toc-perplexity" class="nav-link" data-scroll-target="#perplexity">Perplexity</a></li>
  <li><a href="#goodfire" id="toc-goodfire" class="nav-link" data-scroll-target="#goodfire">Goodfire</a></li>
  <li><a href="#hugging-face" id="toc-hugging-face" class="nav-link" data-scroll-target="#hugging-face">Hugging Face</a>
  <ul class="collapse">
  <li><a href="#batching" id="toc-batching" class="nav-link" data-scroll-target="#batching">Batching</a></li>
  <li><a href="#device" id="toc-device" class="nav-link" data-scroll-target="#device">Device</a></li>
  <li><a href="#hidden-states" id="toc-hidden-states" class="nav-link" data-scroll-target="#hidden-states">Hidden States</a></li>
  <li><a href="#local-models" id="toc-local-models" class="nav-link" data-scroll-target="#local-models">Local Models</a></li>
  </ul></li>
  <li><a href="#vllm" id="toc-vllm" class="nav-link" data-scroll-target="#vllm">vLLM</a>
  <ul class="collapse">
  <li><a href="#batching-1" id="toc-batching-1" class="nav-link" data-scroll-target="#batching-1">Batching</a></li>
  <li><a href="#device-1" id="toc-device-1" class="nav-link" data-scroll-target="#device-1">Device</a></li>
  <li><a href="#local-models-1" id="toc-local-models-1" class="nav-link" data-scroll-target="#local-models-1">Local Models</a></li>
  <li><a href="#tool-use-and-reasoning" id="toc-tool-use-and-reasoning" class="nav-link" data-scroll-target="#tool-use-and-reasoning">Tool Use and Reasoning</a></li>
  <li><a href="#vllm-server" id="toc-vllm-server" class="nav-link" data-scroll-target="#vllm-server">vLLM Server</a></li>
  </ul></li>
  <li><a href="#sglang" id="toc-sglang" class="nav-link" data-scroll-target="#sglang">SGLang</a>
  <ul class="collapse">
  <li><a href="#tool-use-and-reasoning-1" id="toc-tool-use-and-reasoning-1" class="nav-link" data-scroll-target="#tool-use-and-reasoning-1">Tool Use and Reasoning</a></li>
  </ul></li>
  <li><a href="#transformer-lens" id="toc-transformer-lens" class="nav-link" data-scroll-target="#transformer-lens">TransformerLens</a>
  <ul class="collapse">
  <li><a href="#usage-with-pre-loaded-models" id="toc-usage-with-pre-loaded-models" class="nav-link" data-scroll-target="#usage-with-pre-loaded-models">Usage with Pre-loaded Models</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  <li><a href="#ollama" id="toc-ollama" class="nav-link" data-scroll-target="#ollama">Ollama</a></li>
  <li><a href="#llama-cpp-python" id="toc-llama-cpp-python" class="nav-link" data-scroll-target="#llama-cpp-python">Llama-cpp-python</a></li>
  <li><a href="#openai-api" id="toc-openai-api" class="nav-link" data-scroll-target="#openai-api">OpenAI Compatible</a>
  <ul class="collapse">
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  <li><a href="#responses-api-1" id="toc-responses-api-1" class="nav-link" data-scroll-target="#responses-api-1">Responses API</a></li>
  <li><a href="#tool-emulation-openai" id="toc-tool-emulation-openai" class="nav-link" data-scroll-target="#tool-emulation-openai">Tool Emulation</a></li>
  </ul></li>
  <li><a href="#openrouter" id="toc-openrouter" class="nav-link" data-scroll-target="#openrouter">OpenRouter</a></li>
  <li><a href="#hugging-face-inference-providers" id="toc-hugging-face-inference-providers" class="nav-link" data-scroll-target="#hugging-face-inference-providers">Hugging Face Inference Providers</a></li>
  <li><a href="#custom-models" id="toc-custom-models" class="nav-link" data-scroll-target="#custom-models">Custom Models</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/UKGovernmentBEIS/inspect_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./models.html">Models</a></li><li class="breadcrumb-item"><a href="./providers.html">Providers</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Model Providers</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>Inspect has support for a wide variety of language model APIs and can be extended to support arbitrary additional ones. Support for the following providers is built in to Inspect:</p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>Lab APIs</td>
<td><a href="./providers.html#openai">OpenAI</a>, <a href="./providers.html#anthropic">Anthropic</a>, <a href="./providers.html#google">Google</a>, <a href="./providers.html#grok">Grok</a>, <a href="./providers.html#mistral">Mistral</a>, <a href="./providers.html#deepseek">DeepSeek</a>, <a href="./providers.html#perplexity">Perplexity</a></td>
</tr>
<tr class="even">
<td>Cloud APIs</td>
<td><a href="./providers.html#aws-bedrock">AWS Bedrock</a> and <a href="./providers.html#azure-ai">Azure AI</a></td>
</tr>
<tr class="odd">
<td>Open (Hosted)</td>
<td><a href="./providers.html#groq">Groq</a>, <a href="./providers.html#together-ai">Together AI</a>, <a href="./providers.html#fireworks-ai">Fireworks AI</a>, <a href="./providers.html#cloudflare">Cloudflare</a>, <a href="./providers.html#hf-inference-providers">HF Inference Providers</a>, <a href="./providers.html#sambanova">SambaNova</a>, <a href="./providers.html#goodfire">Goodfire</a></td>
</tr>
<tr class="even">
<td>Open (Local)</td>
<td><a href="./providers.html#hugging-face">Hugging Face</a>, <a href="./providers.html#vllm">vLLM</a>, <a href="./providers.html#ollama">Ollama</a>, <a href="./providers.html#llama-cpp-python">Lllama-cpp-python</a>, <a href="./providers.html#sglang">SGLang</a>, <a href="./providers.html#transformer-lens">TransformerLens</a></td>
</tr>
</tbody>
</table>
<p><br></p>
<p>If the provider you are using is not listed above, you may still be able to use it if:</p>
<ol type="1">
<li><p>It provides an OpenAI compatible API endpoint. In this scenario, use the Inspect <a href="./providers.html#openai-api">OpenAI Compatible API</a> interface.</p></li>
<li><p>It is available via OpenRouter (see the docs on using <a href="./providers.html#openrouter">OpenRouter</a> with Inspect).</p></li>
</ol>
<p>You can also create <a href="./extensions.html#model-apis">Model API Extensions</a> to add model providers using their native interface.</p>
</section>
<section id="openai" class="level2">
<h2 class="anchored" data-anchor-id="openai">OpenAI</h2>
<p>To use the <a href="https://platform.openai.com/">OpenAI</a> provider, install the <code>openai</code> package, set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">OPENAI_API_KEY</span><span class="op">=</span>your-openai-api-key</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> openai/gpt-4o-mini</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The following environment variables are supported by the OpenAI provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>OPENAI_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>OPENAI_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.openai.com/v1</code>)</td>
</tr>
<tr class="odd">
<td><code>OPENAI_ORG_ID</code></td>
<td>OpenAI organization ID (optional)</td>
</tr>
<tr class="even">
<td><code>OPENAI_PROJECT_ID</code></td>
<td>OpenAI project ID (optional)</td>
</tr>
</tbody>
</table>
<section id="model-args" class="level3">
<h3 class="anchored" data-anchor-id="model-args">Model Args</h3>
<p>The <code>openai</code> provider supports the following custom model args (other model args are forwarded to the constructor of the <code>AsyncOpenAI</code> class):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 35%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Model Arg</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>responses_api</code></td>
<td>Use the OpenAI Responses API rather than the Chat Completions API.</td>
</tr>
<tr class="even">
<td><code>responses_store</code></td>
<td>Pass <code>store=True</code> to the Responses API (defaults to <code>True</code>).</td>
</tr>
<tr class="odd">
<td><code>service_tier</code></td>
<td>Processing type used for serving the request (“auto”, “default”, or “flex”).</td>
</tr>
<tr class="even">
<td><code>background</code></td>
<td>Execute generate requests asynchronously, polling response objects to check status over time. Defaults to <code>True</code> for <code>gpt-5-pro</code> and <code>deep-research</code> and <code>False</code> for other models.</td>
</tr>
<tr class="odd">
<td><code>safety_identifier</code></td>
<td>A stable identifier used to help detect users of your application.</td>
</tr>
<tr class="even">
<td><code>prompt_cache_key</code></td>
<td>Used by OpenAI to cache responses for similar requests.</td>
</tr>
<tr class="odd">
<td><code>http_client</code></td>
<td>Custom instance of <code>httpx.AsyncClient</code> for handling requests.</td>
</tr>
</tbody>
</table>
<p>For example:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> openai/gpt-4o-mini <span class="dt">\ </span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>   <span class="ex">-M</span> responses_api=true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or from Python:</p>
<div id="249cee09" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">eval</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"arc.py"</span>, model<span class="op">=</span><span class="st">" openai/gpt-4o-mini"</span>, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    model_args<span class="op">=</span> { <span class="st">"responses_api"</span>: <span class="va">True</span> }</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="responses-api" class="level3">
<h3 class="anchored" data-anchor-id="responses-api">Responses API</h3>
<p>By default, Inspect uses the standard OpenAI Chat Completions API for GPT-4 models and the new <a href="https://platform.openai.com/docs/api-reference/responses">Responses API</a> for GPT-5 and o-series models and the <code>computer_use_preview</code> model.</p>
<p>If you want to manually enable or disable the Responses API you can use the <code>responses_api</code> model argument. For example:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval math.py <span class="at">--model</span> openai/gpt-4o <span class="at">-M</span> responses_api=true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that certain models including <code>o1-pro</code> and <code>computer_use_preview</code> <em>require</em> the use of the Responses API. Check the Open AI <a href="https://platform.openai.com/docs/models">models documentation</a> for details on which models are supported by the respective APIs.</p>
</section>
<section id="responses-store" class="level3">
<h3 class="anchored" data-anchor-id="responses-store">Responses Store</h3>
<p>By default, the Responses API stores requests on the server for retrieval of previous reasoning content (which is not transmitted as part of responses). To control this behavior explicitly use the <code>responses_store</code> model argument. For example:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval math.py <span class="at">--model</span> openai/o4-mini <span class="at">-M</span> responses_store=false</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For example, you might need to do this if you have a non-logging interface to OpenAI models (as <code>store</code> is incompatible with non-logging interfaces). Note that some features (such as computer use) <em>require</em> responses store to be <code>True</code>.</p>
</section>
<section id="flex-processing" class="level3">
<h3 class="anchored" data-anchor-id="flex-processing">Flex Processing</h3>
<p><a href="https://platform.openai.com/docs/guides/flex-processing">Flex processing</a> provides significantly lower costs for requests in exchange for slower response times and occasional resource unavailability (input and output tokens are priced using <a href="https://platform.openai.com/docs/guides/batch">batch API rates</a> for flex requests).</p>
<p>Note that flex processing is in beta, and currently <strong>only available for o3 and o4-mini models</strong>.</p>
<p>To enable flex processing, use the <code>service_tier</code> model argument, setting it to “flex”. For example:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval math.py <span class="at">--model</span> openai/o4-mini <span class="at">-M</span> service_tier=flex</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>OpenAI recommends using a <a href="https://platform.openai.com/docs/guides/flex-processing#api-request-timeouts">higher client timeout</a> when making flex requests (15 minutes rather than the standard 10). Inspect automatically increases the client timeout to 15 minutes (900 seconds) for flex requests. To specify another value, use the <code>client_timeout</code> model argument. For example:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval math.py <span class="at">--model</span> openai/o4-mini <span class="dt">\</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">-M</span> service_tier=flex <span class="at">-M</span> client_timeout=1200</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="openai-on-azure" class="level3">
<h3 class="anchored" data-anchor-id="openai-on-azure">OpenAI on Azure</h3>
<p>The <code>openai</code> provider supports OpenAI models deployed on the <a href="https://ai.azure.com/">Azure AI Foundry</a>. To use OpenAI models on Azure AI, specify the following environment variables:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 62%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>AZUREAI_OPENAI_API_KEY</code></td>
<td>API key credentials (optional).</td>
</tr>
<tr class="even">
<td><code>AZUREAI_OPENAI_BASE_URL</code></td>
<td>Base URL for requests (required)</td>
</tr>
<tr class="odd">
<td><code>AZUREAI_OPENAI_API_VERSION</code></td>
<td>OpenAI API version (optional)</td>
</tr>
<tr class="even">
<td><code>AZUREAI_AUDIENCE</code></td>
<td>Azure resource URI that the access token is intended for when using managed identity (optional, defaults to <code>https://cognitiveservices.azure.com/.default</code>)</td>
</tr>
</tbody>
</table>
<p>You can then use the normal <code>openai</code> provider with the <code>azure</code> qualifier and the name of your model deployment (e.g.&nbsp;<code>gpt-4o-mini</code>). For example:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_OPENAI_API_KEY</span><span class="op">=</span>your-api-key</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_OPENAI_BASE_URL</span><span class="op">=</span>https://your-url-at.azure.com</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_OPENAI_API_VERSION</span><span class="op">=</span>2025-03-01-preview</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval math.py <span class="at">--model</span> openai/azure/gpt-4o-mini</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If using managed identity for authentication, install the <code>azure-identity</code> package and do not specify <code>AZUREAI_API_KEY</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install azure-identity</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_OPENAI_BASE_URL</span><span class="op">=</span>https://your-url-at.azure.com</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_AUDIENCE</span><span class="op">=</span>https://cognitiveservices.azure.com/.default</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_OPENAI_API_VERSION</span><span class="op">=</span>2025-03-01-preview</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval math.py <span class="at">--model</span> openai/azure/gpt-4o-mini</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that if the <code>AZUREAI_OPENAI_API_VERSION</code> is not specified, Inspect will generally default to the latest deployed version, which as of this writing is <code>2025-03-01-preview</code>. When using managed identity for authentication, install the <code>azure-identity</code> package and leave <code>AZUREAI_OPENAI_API_KEY</code> undefined.</p>
</section>
</section>
<section id="anthropic" class="level2">
<h2 class="anchored" data-anchor-id="anthropic">Anthropic</h2>
<p>To use the <a href="https://www.anthropic.com/api">Anthropic</a> provider, install the <code>anthropic</code> package, set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install anthropic</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">ANTHROPIC_API_KEY</span><span class="op">=</span>your-anthropic-api-key</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> anthropic/claude-sonnet-4-0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>anthropic</code> provider, custom model args (<code>-M</code>) are forwarded to the constructor of the <code>AsyncAnthropic</code> class.</p>
<p>The following environment variables are supported by the Anthropic provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ANTHROPIC_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>ANTHROPIC_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.anthropic.com</code>)</td>
</tr>
</tbody>
</table>
<section id="betas" class="level3">
<h3 class="anchored" data-anchor-id="betas">Betas</h3>
<p>Some Anthropic features require that you include a beta identifier in the <code>betas</code> field of model requests. Inspect automatically includes the requisite identifier for beta features it utilizes (e.g.&nbsp;“mcp-client-2025-04-04”, “computer-use-2025-01-24”, etc.).</p>
<p>If there are other beta features you want to enable, use the <code>betas</code> model arg (<code>-M</code>). For example, to enable <a href="https://docs.anthropic.com/en/docs/build-with-claude/context-windows#1m-token-context-window">1M token context windows</a> for Sonnet 5 models:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> anthropic/claude-sonnet-4-0 <span class="at">-M</span> betas=context-1m-2025-08-07</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="streaming" class="level3">
<h3 class="anchored" data-anchor-id="streaming">Streaming</h3>
<p>The Anthropic provider supports a <code>streaming</code> model arg (<code>-M</code>) that controls whether streaming responses are used. The default (“auto”) will automatically use streaming when thinking is enabled or for potentially <a href="https://github.com/anthropics/anthropic-sdk-python?tab=readme-ov-file#long-requests">long requests</a> (requests with &gt;= 8192 <code>max_tokens</code>). Pass <code>true</code> or <code>false</code> to override the default behavior:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> anthropic/claude-sonnet-4-0 <span class="at">-M</span> streaming=true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="anthropic-on-aws-bedrock" class="level3">
<h3 class="anchored" data-anchor-id="anthropic-on-aws-bedrock">Anthropic on AWS Bedrock</h3>
<p>To use Anthropic models on Bedrock, use the normal <code>anthropic</code> provider with the <code>bedrock</code> qualifier, specifying a model name that corresponds to a model you have access to on Bedrock. For Bedrock, authentication is not handled using an API key but rather your standard AWS credentials (e.g.&nbsp;<code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>). You should also be sure to have specified an AWS region. For example:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AWS_ACCESS_KEY_ID</span><span class="op">=</span>your-aws-access-key-id</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AWS_SECRET_ACCESS_KEY</span><span class="op">=</span>your-aws-secret-access-key</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AWS_DEFAULT_REGION</span><span class="op">=</span>us-east-1</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> anthropic/bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can also optionally set the <code>ANTHROPIC_BEDROCK_BASE_URL</code> environment variable to set a custom base URL for Bedrock API requests.</p>
</section>
<section id="anthropic-on-vertex-ai" class="level3">
<h3 class="anchored" data-anchor-id="anthropic-on-vertex-ai">Anthropic on Vertex AI</h3>
<p>To use Anthropic models on Vertex, you can use the standard <code>anthropic</code> model provider with the <code>vertex</code> qualifier (e.g.&nbsp;<code>anthropic/vertex/claude-3-5-sonnet-v2@20241022</code>). You should also set two environment variables indicating your project ID and region. Here is a complete example:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">ANTHROPIC_VERTEX_PROJECT_ID</span><span class="op">=</span>project-12345</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">ANTHROPIC_VERTEX_REGION</span><span class="op">=</span>us-east5</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval ctf.py <span class="at">--model</span> anthropic/vertex/claude-3-5-sonnet-v2@20241022</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Authentication is doing using the standard Google Cloud CLI (i.e.&nbsp;if you have authorised the CLI then no additional auth is needed for the model API).</p>
</section>
</section>
<section id="google-gemini" class="level2">
<h2 class="anchored" data-anchor-id="google-gemini">Google</h2>
<p>To use the <a href="https://ai.google.dev/">Google</a> provider, install the <code>google-genai</code> package, set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install google-genai</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">GOOGLE_API_KEY</span><span class="op">=</span>your-google-api-key</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> google/gemini-2.5-pro</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>google</code> provider, custom model args (<code>-M</code>) are forwarded to the <code>genai.Client</code> function.</p>
<p>The following environment variables are supported by the Google provider</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>GOOGLE_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>GOOGLE_BASE_URL</code></td>
<td>Base URL for requests (optional)</td>
</tr>
</tbody>
</table>
<section id="gemini-on-vertex-ai" class="level3">
<h3 class="anchored" data-anchor-id="gemini-on-vertex-ai">Gemini on Vertex AI</h3>
<p>To use Google Gemini models on Vertex, you can use the standard <code>google</code> model provider with the <code>vertex</code> qualifier (e.g.&nbsp;<code>google/vertex/gemini-2.0-flash</code>). You should also set two environment variables indicating your project ID and region. Here is a complete example:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">GOOGLE_CLOUD_PROJECT</span><span class="op">=</span>project-12345</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">GOOGLE_CLOUD_LOCATION</span><span class="op">=</span>us-east5</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval ctf.py <span class="at">--model</span> google/vertex/gemini-2.0-flash</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can alternatively pass the project and location as custom model args (<code>-M</code>). For example:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval ctf.py <span class="at">--model</span> google/vertex/gemini-2.0-flash <span class="dt">\</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>   <span class="at">-M</span> project=project-12345 <span class="at">-M</span> location=us-east5</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Authentication is done using the standard Google Cloud CLI. For example:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">gcloud</span> auth application-default login</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you have authorised the CLI then no additional auth is needed for the model API.</p>
<p>You can optionally specify a custom <code>GOOGLE_VERTEX_BASE_URL</code> to override the default base URL for Vertex.</p>
</section>
<section id="safety-settings" class="level3">
<h3 class="anchored" data-anchor-id="safety-settings">Safety Settings</h3>
<p>Google models make available <a href="https://ai.google.dev/gemini-api/docs/safety-settings">safety settings</a> that you can adjust to determine what sorts of requests will be handled (or refused) by the model. The five categories of safety settings are as follows:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 35%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Category</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>civic_integrity</code></td>
<td>Election-related queries.</td>
</tr>
<tr class="even">
<td><code>sexually_explicit</code></td>
<td>Contains references to sexual acts or other lewd content.</td>
</tr>
<tr class="odd">
<td><code>hate_speech</code></td>
<td>Content that is rude, disrespectful, or profane.</td>
</tr>
<tr class="even">
<td><code>harassment</code></td>
<td>Negative or harmful comments targeting identity and/or protected attributes.</td>
</tr>
<tr class="odd">
<td><code>dangerous_content</code></td>
<td>Promotes, facilitates, or encourages harmful acts.</td>
</tr>
</tbody>
</table>
<p>For each category, the following block thresholds are available:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 35%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Block Threshold</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>none</code></td>
<td>Always show regardless of probability of unsafe content</td>
</tr>
<tr class="even">
<td><code>only_high</code></td>
<td>Block when high probability of unsafe content</td>
</tr>
<tr class="odd">
<td><code>medium_and_above</code></td>
<td>Block when medium or high probability of unsafe content</td>
</tr>
<tr class="even">
<td><code>low_and_above</code></td>
<td>Block when low, medium or high probability of unsafe content</td>
</tr>
</tbody>
</table>
<p>By default, Inspect sets all four categories to <code>none</code> (enabling all content). You can override these defaults by using the <code>safety_settings</code> model argument. For example:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>safety_settings <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  dangerous_content <span class="op">=</span> <span class="st">"medium_and_above"</span>,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  hate_speech <span class="op">=</span> <span class="st">"low_and_above"</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="bu">eval</span>(</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">"eval.py"</span>,</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  model_args<span class="op">=</span><span class="bu">dict</span>(safety_settings<span class="op">=</span>safety_settings)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This also can be done from the command line:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval eval.py <span class="at">-M</span> <span class="st">"safety_settings={'hate_speech': 'low_and_above'}"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="mistral" class="level2">
<h2 class="anchored" data-anchor-id="mistral">Mistral</h2>
<p>To use the <a href="https://mistral.ai/">Mistral</a> provider, install the <code>mistral</code> package, set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install mistral</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">MISTRAL_API_KEY</span><span class="op">=</span>your-mistral-api-key</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> mistral/mistral-large-latest</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>mistral</code> provider, custom model args (<code>-M</code>) are forwarded to the constructor of the <code>Mistral</code> class.</p>
<p>The following environment variables are supported by the Mistral provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>MISTRAL_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>MISTRAL_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.mistral.ai</code>)</td>
</tr>
</tbody>
</table>
<section id="mistral-on-azure-ai" class="level3">
<h3 class="anchored" data-anchor-id="mistral-on-azure-ai">Mistral on Azure AI</h3>
<p>The <code>mistral</code> provider supports Mistral models deployed on the <a href="https://ai.azure.com/">Azure AI Foundry</a>. To use Mistral models on Azure AI, specify the following environment variables:</p>
<ul>
<li><code>AZURE_MISTRAL_API_KEY</code></li>
<li><code>AZUREAI_MISTRAL_BASE_URL</code></li>
</ul>
<p>You can then use the normal <code>mistral</code> provider with the <code>azure</code> qualifier and the name of your model deployment (e.g.&nbsp;<code>Mistral-Large-2411</code>). For example:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_MISTRAL_API_KEY</span><span class="op">=</span>key</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_MISTRAL_BASE_URL</span><span class="op">=</span>https://your-url-at.azure.com/models</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval math.py <span class="at">--model</span> mistral/azure/Mistral-Large-2411</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="deepseek" class="level2">
<h2 class="anchored" data-anchor-id="deepseek">DeepSeek</h2>
<p><a href="https://www.deepseek.com/">DeepSeek</a> provides an OpenAI compatible API endpoint which you can use with Inspect via the <code>openai-api</code> provider. To do this, define the <code>DEEPSEEK_API_KEY</code> and <code>DEEPSEEK_BASE_URL</code> environment variables then refer to models with <code>openai-api/deepseek/&lt;model-name&gt;</code>. For example:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DEEPSEEK_API_KEY</span><span class="op">=</span>your-deepseek-api-key</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DEEPSEEK_BASE_URL</span><span class="op">=</span>https://api.deepseek.com</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> openai-api/deepseek/deepseek-reasoner</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="grok" class="level2">
<h2 class="anchored" data-anchor-id="grok">Grok</h2>
<p>To use the <a href="https://x.ai/">Grok</a> provider, install the <code>openai</code> package (which the Grok service provides a compatible backend for), set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">GROK_API_KEY</span><span class="op">=</span>your-grok-api-key</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> grok/grok-3-mini</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>grok</code> provider, custom model args (<code>-M</code>) are forwarded to the constructor of the <code>AsyncOpenAI</code> class.</p>
<p>The following environment variables are supported by the Grok provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>GROK_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>GROK_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.x.ai/v1</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="aws-bedrock" class="level2">
<h2 class="anchored" data-anchor-id="aws-bedrock">AWS Bedrock</h2>
<p>To use the <a href="https://aws.amazon.com/bedrock/">AWS Bedrock</a> provider, install the <code>aioboto3</code> package, set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AWS_ACCESS_KEY_ID</span><span class="op">=</span>access-key-id</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AWS_SECRET_ACCESS_KEY</span><span class="op">=</span>secret-access-key</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AWS_DEFAULT_REGION</span><span class="op">=</span>us-east-1</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval bedrock/meta.llama2-70b-chat-v1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>bedrock</code> provider, custom model args (<code>-M</code>) are forwarded to the <code>client</code> method of the <code>aioboto3.Session</code> class.</p>
<p>Note that all models on AWS Bedrock require that you <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html">request model access</a> before using them in a deployment (in some cases access is granted immediately, in other cases it could one or more days).</p>
<p>You should be also sure that you have the appropriate AWS credentials before accessing models on Bedrock. You aren’t likely to need to, but you can also specify a custom base URL for AWS Bedrock using the <code>BEDROCK_BASE_URL</code> environment variable.</p>
<p>If you are using Anthropic models on Bedrock, you can alternatively use the <a href="#anthropic-on-aws-bedrock">Anthropic provider</a> as your means of access.</p>
</section>
<section id="azure-ai" class="level2">
<h2 class="anchored" data-anchor-id="azure-ai">Azure AI</h2>
<p>The <code>azureai</code> provider supports models deployed on the <a href="https://ai.azure.com/">Azure AI Foundry</a>.</p>
<p>To use the <code>azureai</code> provider, install the <code>azure-ai-inference</code> package, set your credentials and base URL, and specify the name of the model you have deployed (e.g.&nbsp;<code>Llama-3.3-70B-Instruct</code>). For example:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install azure-ai-inference</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_API_KEY</span><span class="op">=</span>api-key</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_BASE_URL</span><span class="op">=</span>https://your-url-at.azure.com/models</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval math.py <span class="at">--model</span> azureai/Llama-3.3-70B-Instruct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If using managed identity for authentication, install the <code>azure-identity</code> package and do not specify <code>AZUREAI_API_KEY</code>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install azure-identity</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_AUDIENCE</span><span class="op">=</span>https://cognitiveservices.azure.com/.default</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">AZUREAI_BASE_URL</span><span class="op">=</span>https://your-url-at.azure.com/models</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval math.py <span class="at">--model</span> azureai/Llama-3.3-70B-Instruct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>azureai</code> provider, custom model args (<code>-M</code>) are forwarded to the constructor of the <code>ChatCompletionsClient</code> class.</p>
<p>The following environment variables are supported by the Azure AI provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 62%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>AZUREAI_API_KEY</code></td>
<td>API key credentials (optional).</td>
</tr>
<tr class="even">
<td><code>AZUREAI_BASE_URL</code></td>
<td>Base URL for requests (required)</td>
</tr>
<tr class="odd">
<td><code>AZUREAI_AUDIENCE</code></td>
<td>Azure resource URI that the access token is intended for when using managed identity (optional, defaults to <code>https://cognitiveservices.azure.com/.default</code>)</td>
</tr>
</tbody>
</table>
<p>If you are using Open AI or Mistral on Azure AI, you can alternatively use the <a href="#openai-on-azure">OpenAI provider</a> or <a href="#mistral-on-azure-ai">Mistral provider</a> as your means of access.</p>
<section id="tool-emulation" class="level3">
<h3 class="anchored" data-anchor-id="tool-emulation">Tool Emulation</h3>
<p>When using the <code>azureai</code> model provider, tool calling support can be ‘emulated’ for models that Azure AI has not yet implemented tool calling for. This occurs by default for Llama models. For other models, use the <code>emulate_tools</code> model arg to force tool emulation:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval ctf.py <span class="at">-M</span> emulate_tools=true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can also use this option to disable tool emulation for Llama models with <code>emulate_tools=false</code>.</p>
</section>
</section>
<section id="together-ai" class="level2">
<h2 class="anchored" data-anchor-id="together-ai">Together AI</h2>
<p>To use the <a href="https://www.together.ai/">Together AI</a> provider, install the <code>openai</code> package (which the Together AI service provides a compatible backend for), set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">TOGETHER_API_KEY</span><span class="op">=</span>your-together-api-key</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> together/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>together</code> provider, you can enable <a href="#tool-emulation-openai">Tool Emulation</a> using the <code>emulate_tools</code> custom model arg (<code>-M</code>). Other custom model args are forwarded to the constructor of the <code>AsyncOpenAI</code> class.</p>
<p>The following environment variables are supported by the Together AI provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>TOGETHER_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>TOGETHER_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.together.xyz/v1</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="groq" class="level2">
<h2 class="anchored" data-anchor-id="groq">Groq</h2>
<p>To use the <a href="https://groq.com/">Groq</a> provider, install the <code>groq</code> package, set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install groq</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">GROQ_API_KEY</span><span class="op">=</span>your-groq-api-key</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> groq/llama-3.1-70b-versatile</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>groq</code> provider, custom model args (<code>-M</code>) are forwarded to the constructor of the <code>AsyncGroq</code> class.</p>
<p>The following environment variables are supported by the Groq provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>GROQ_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>GROQ_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.groq.com</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="fireworks-ai" class="level2">
<h2 class="anchored" data-anchor-id="fireworks-ai">Fireworks AI</h2>
<p>To use the <a href="https://fireworks.ai/">Fireworks AI</a> provider, install the <code>openai</code> package (which the Fireworks AI service provides a compatible backend for), set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">FIREWORKS_API_KEY</span><span class="op">=</span>your-firewrks-api-key</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> fireworks/accounts/fireworks/models/deepseek-r1-0528</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>fireworks</code> provider, you can enable <a href="#tool-emulation-openai">Tool Emulation</a> using the <code>emulate_tools</code> custom model arg (<code>-M</code>). Other custom model args are forwarded to the constructor of the <code>AsyncOpenAI</code> class.</p>
<p>The following environment variables are supported by the Together AI provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>FIREWORKS_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>FIREWORKS_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.fireworks.ai/inference/v1</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="sambanova" class="level2">
<h2 class="anchored" data-anchor-id="sambanova">SambaNova</h2>
<p>To use the <a href="https://sambanova.ai/">SambaNova</a> provider, install the <code>openai</code> package (which the SambaNova service provides a compatible backend for), set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">SAMABANOVA_API_KEY</span><span class="op">=</span>your-sambanova-api-key</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> sambanova/DeepSeek-V1-0324</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>sambanova</code> provider, you can enable <a href="#tool-emulation-openai">Tool Emulation</a> using the <code>emulate_tools</code> custom model arg (<code>-M</code>). Other custom model args are forwarded to the constructor of the <code>AsyncOpenAI</code> class.</p>
<p>The following environment variables are supported by the SambaNova provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>SAMBANOVA_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>SAMBANOVA_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.sambanova.ai/v1</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="cloudflare" class="level2">
<h2 class="anchored" data-anchor-id="cloudflare">Cloudflare</h2>
<p>To use the <a href="https://developers.cloudflare.com/workers-ai/">Cloudflare</a> provider, set your account id and access token, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">CLOUDFLARE_ACCOUNT_ID</span><span class="op">=</span>account-id</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">CLOUDFLARE_API_TOKEN</span><span class="op">=</span>api-token</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> cf/meta/llama-3.1-70b-instruct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>cloudflare</code> provider, custom model args (<code>-M</code>) are included as fields in the post body of the chat request.</p>
<p>The following environment variables are supported by the Cloudflare provider:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>CLOUDFLARE_ACCOUNT_ID</code></td>
<td>Account id (required).</td>
</tr>
<tr class="even">
<td><code>CLOUDFLARE_API_TOKEN</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="odd">
<td><code>CLOUDFLARE_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.cloudflare.com/client/v4/accounts</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="perplexity" class="level2">
<h2 class="anchored" data-anchor-id="perplexity">Perplexity</h2>
<p>To use the <a href="https://www.perplexity.ai/">Perplexity</a> provider, install the <code>openai</code> package (if not already installed), set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">PERPLEXITY_API_KEY</span><span class="op">=</span>your-perplexity-api-key</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> perplexity/sonar</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The following environment variables are supported by the Perplexity provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>PERPLEXITY_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>PERPLEXITY_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.perplexity.ai</code>)</td>
</tr>
</tbody>
</table>
<p>Perplexity responses include citations when available. These are surfaced as <span class="element-type-name ref-interlink"><a href="./reference/inspect_ai.model.html#urlcitation">UrlCitation</a></span>s attached to the assistant message. Additional usage metrics such as <code>reasoning_tokens</code> and <code>citation_tokens</code> are recorded in <code>ModelOutput.metadata</code>.</p>
</section>
<section id="goodfire" class="level2">
<h2 class="anchored" data-anchor-id="goodfire">Goodfire</h2>
<p>To use the <a href="https://platform.goodfire.ai/">Goodfire</a> provider, install the <code>goodfire</code> package, set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install goodfire</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">GOODFIRE_API_KEY</span><span class="op">=</span>your-goodfire-api-key</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> goodfire/meta-llama/Meta-Llama-3.1-8B-Instruct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>goodfire</code> provider, custom model args (<code>-M</code>) are forwarded to <code>chat.completions.create</code> method of the <code>AsyncClient</code> class.</p>
<p>The following environment variables are supported by the Goodfire provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>GOODFIRE_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>GOODFIRE_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://api.goodfire.ai</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="hugging-face" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face">Hugging Face</h2>
<p>The <a href="https://huggingface.co/models">Hugging Face</a> provider implements support for local models using the <a href="https://pypi.org/project/transformers/">transformers</a> package. To use the Hugging Face provider, install the <code>torch</code>, <code>transformers</code>, and <code>accelerate</code> packages and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch transformers accelerate</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> hf/openai-community/gpt2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="batching" class="level3">
<h3 class="anchored" data-anchor-id="batching">Batching</h3>
<p>Concurrency for REST API based models is managed using the <code>max_connections</code> option. The same option is used for <code>transformers</code> inference—up to <code>max_connections</code> calls to <span class="element-type-name ref-interlink"><a href="./reference/inspect_ai.solver.html#generate">generate()</a></span> will be batched together (note that batches will proceed at a smaller size if no new calls to <span class="element-type-name ref-interlink"><a href="./reference/inspect_ai.solver.html#generate">generate()</a></span> have occurred in the last 2 seconds).</p>
<p>The default batch size for Hugging Face is 32, but you should tune your <code>max_connections</code> to maximise performance and ensure that batches don’t exceed available GPU memory. The <a href="https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching">Pipeline Batching</a> section of the transformers documentation is a helpful guide to the ways batch size and performance interact.</p>
</section>
<section id="device" class="level3">
<h3 class="anchored" data-anchor-id="device">Device</h3>
<p>The PyTorch <code>cuda</code> device will be used automatically if CUDA is available (as will the Mac OS <code>mps</code> device). If you want to override the device used, use the <code>device</code> model argument. For example:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> hf/openai-community/gpt2 <span class="at">-M</span> device=cuda:0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This also works in calls to <span class="element-type-name ref-interlink"><a href="./reference/inspect_ai.html#eval">eval()</a></span>:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">eval</span>(<span class="st">"arc.py"</span>, model<span class="op">=</span><span class="st">"hf/openai-community/gpt2"</span>, model_args<span class="op">=</span><span class="bu">dict</span>(device<span class="op">=</span><span class="st">"cuda:0"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or in a call to <span class="element-type-name ref-interlink"><a href="./reference/inspect_ai.model.html#get_model">get_model()</a></span></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(<span class="st">"hf/openai-community/gpt2"</span>, device<span class="op">=</span><span class="st">"cuda:0"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="hidden-states" class="level3">
<h3 class="anchored" data-anchor-id="hidden-states">Hidden States</h3>
<p>If you wish to access hidden states (activations) from generation, use the <code>hidden_states</code> model arg. For example:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> hf/openai-community/gpt2 <span class="at">-M</span> hidden_states=true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or from Python:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"hf/meta-llama/Llama-3.1-8B-Instruct"</span>,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    hidden_states<span class="op">=</span><span class="va">True</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Activations are available in the “hidden_states” field of <code>ModelOutput.metadata</code>. The hidden_states value is the same as transformers <a href="https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput">GenerateDecoderOnlyOutput</a>.</p>
</section>
<section id="local-models" class="level3">
<h3 class="anchored" data-anchor-id="local-models">Local Models</h3>
<p>In addition to using models from the Hugging Face Hub, the Hugging Face provider can also use local model weights and tokenizers (e.g.&nbsp;for a locally fine tuned model). Use <code>hf/local</code> along with the <code>model_path</code>, and (optionally) <code>tokenizer_path</code> arguments to select a local model. For example, from the command line, use the <code>-M</code> flag to pass the model arguments:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> hf/local <span class="at">-M</span> model_path=./my-model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or using the <span class="element-type-name ref-interlink"><a href="./reference/inspect_ai.html#eval">eval()</a></span> function:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="bu">eval</span>(<span class="st">"arc.py"</span>, model<span class="op">=</span><span class="st">"hf/local"</span>, model_args<span class="op">=</span><span class="bu">dict</span>(model_path<span class="op">=</span><span class="st">"./my-model"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or in a call to <span class="element-type-name ref-interlink"><a href="./reference/inspect_ai.model.html#get_model">get_model()</a></span></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(<span class="st">"hf/local"</span>, model_path<span class="op">=</span><span class="st">"./my-model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="vllm" class="level2">
<h2 class="anchored" data-anchor-id="vllm">vLLM</h2>
<p>The <a href="https://docs.vllm.ai/">vLLM</a> provider also implements support for Hugging Face models using the <a href="https://github.com/vllm-project/vllm/">vllm</a> package. To use the vLLM provider, install the <code>vllm</code> package and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install vllm</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> vllm/openai-community/gpt2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>vllm</code> provider, custom model args (-M) are forwarded to the vllm <a href="https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#cli-reference">CLI</a>.</p>
<p>The following environment variables are supported by the vLLM provider:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>VLLM_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to the server started by Inspect)</td>
</tr>
<tr class="even">
<td><code>VLLM_API_KEY</code></td>
<td>API key for the vLLM server (optional, defaults to “local”)</td>
</tr>
<tr class="odd">
<td><code>VLLM_DEFAULT_SERVER_ARGS</code></td>
<td>JSON string of default server args (e.g., ‘{“tensor_parallel_size”: 4, “max_model_len”: 8192}’)</td>
</tr>
</tbody>
</table>
<p>You can also access models from ModelScope rather than Hugging Face, see the <a href="https://docs.vllm.ai/en/stable/getting_started/quickstart.html">vLLM documentation</a> for details on this.</p>
<p>vLLM is generally much faster than the Hugging Face provider as the library is designed entirely for inference speed whereas the Hugging Face library is more general purpose.</p>
<section id="batching-1" class="level3">
<h3 class="anchored" data-anchor-id="batching-1">Batching</h3>
<p>vLLM automatically handles batching, so you generally don’t have to worry about selecting the optimal batch size. However, you can still use the <code>max_connections</code> option to control the number of concurrent requests which defaults to 32.</p>
</section>
<section id="device-1" class="level3">
<h3 class="anchored" data-anchor-id="device-1">Device</h3>
<p>The <code>device</code> option is also available for vLLM models, and you can use it to specify the device(s) to run the model on. For example:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> vllm/meta-llama/Meta-Llama-3-8B-Instruct <span class="at">-M</span> device=<span class="st">'0,1,2,3'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="local-models-1" class="level3">
<h3 class="anchored" data-anchor-id="local-models-1">Local Models</h3>
<p>Similar to the Hugging Face provider, you can also use local models with the vLLM provider. Use <code>vllm/local</code> along with the <code>model_path</code>, and (optionally) <code>tokenizer_path</code> arguments to select a local model. For example, from the command line, use the <code>-M</code> flag to pass the model arguments:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> vllm/local <span class="at">-M</span> model_path=./my-model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="tool-use-and-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="tool-use-and-reasoning">Tool Use and Reasoning</h3>
<p>vLLM supports tool use and reasoning; however, the usage is often model dependant and requires additional configuration. See the <a href="https://docs.vllm.ai/en/stable/features/tool_calling.html">Tool Use</a> and <a href="https://docs.vllm.ai/en/stable/features/reasoning_outputs.html">Reasoning</a> sections of the vLLM documentation for details.</p>
</section>
<section id="vllm-server" class="level3">
<h3 class="anchored" data-anchor-id="vllm-server">vLLM Server</h3>
<p>Rather than letting Inspect start and stop a vLLM server every time you run an evaluation (which can take several minutes for large models), you can instead start the server manually and then connect to it. To do this, set the model base URL to point to the vLLM server and the API key to the server’s API key. For example:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> export VLLM_BASE_URL=http://localhost:8080/v1</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> export VLLM_API_KEY=<span class="op">&lt;</span>your-server-api-key<span class="op">&gt;</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> vllm/meta-llama/Meta-Llama-3-8B-Instruct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>or</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> vllm/meta-llama/Meta-Llama-3-8B-Instruct <span class="at">--model-base-url</span> http://localhost:8080/v1 <span class="at">-M</span> api_key=<span class="op">&lt;</span>your-server-api-key<span class="op">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>See the vLLM documentation on <a href="https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html">Server Mode</a> for additional details.</p>
</section>
</section>
<section id="sglang" class="level2">
<h2 class="anchored" data-anchor-id="sglang">SGLang</h2>
<p>To use the <a href="https://docs.sglang.ai/index.html">SGLang</a> provider, install the <code>sglang</code> package and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="st">"sglang[all]&gt;=0.4.4.post2"</span> <span class="at">--find-links</span> https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> sglang/meta-llama/Meta-Llama-3-8B-Instruct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>sglang</code> provider, custom model args (-M) are forwarded to the sglang <a href="https://docs.sglang.ai/backend/server_arguments.html">CLI</a>.</p>
<p>The following environment variables are supported by the SGLang provider:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>SGLANG_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to the server started by Inspect)</td>
</tr>
<tr class="even">
<td><code>SGLANG_API_KEY</code></td>
<td>API key for the SGLang server (optional, defaults to “local”)</td>
</tr>
<tr class="odd">
<td><code>SGLANG_DEFAULT_SERVER_ARGS</code></td>
<td>JSON string of default server args (e.g., ‘{“tp”: 4, “max_model_len”: 8192}’)</td>
</tr>
</tbody>
</table>
<p>SGLang is a fast and efficient language model server that supports a variety of model architectures and configurations. Its usage in Inspect is almost identical to the <a href="#vllm">vLLM provider</a>. You can either let Inspect start and stop the server for you, or start the server manually and then connect to it:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> export SGLANG_BASE_URL=http://localhost:8080/v1</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> export SGLANG_API_KEY=<span class="op">&lt;</span>your-server-api-key<span class="op">&gt;</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> sglang/meta-llama/Meta-Llama-3-8B-Instruct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>or</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> sglang/meta-llama/Meta-Llama-3-8B-Instruct <span class="at">--model-base-url</span> http://localhost:8080/v1 <span class="at">-M</span> api_key=<span class="op">&lt;</span>your-server-api-key<span class="op">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="tool-use-and-reasoning-1" class="level3">
<h3 class="anchored" data-anchor-id="tool-use-and-reasoning-1">Tool Use and Reasoning</h3>
<p>SGLang supports tool use and reasoning; however, the usage is often model dependant and requires additional configuration. See the <a href="https://docs.sglang.ai/backend/function_calling.html">Tool Use</a> and <a href="https://docs.sglang.ai/backend/separate_reasoning.html">Reasoning</a> sections of the SGLang documentation for details.</p>
</section>
</section>
<section id="transformer-lens" class="level2">
<h2 class="anchored" data-anchor-id="transformer-lens">TransformerLens</h2>
<p>The <a href="https://github.com/neelnanda-io/TransformerLens">TransformerLens</a> provider allows you to use <code>HookedTransformer</code> models with Inspect.</p>
<p>To use the TransformerLens provider, install the <code>transformer_lens</code> package:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install transformer_lens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="usage-with-pre-loaded-models" class="level3">
<h3 class="anchored" data-anchor-id="usage-with-pre-loaded-models">Usage with Pre-loaded Models</h3>
<p>Unlike other providers, TransformerLens requires you to first load a <code>HookedTransformer</code> model instance and then pass it to Inspect. This is because TransformerLens models expose special hooks for accessing and manipulating internal activations that need to be set up before use in the inspect framework.</p>
<p>You will need to specify the <code>tl_model</code> and <code>tl_generate_args</code> in the model arguments. The <code>tl_model</code> is the <code>HookedTransformer</code> instance and the <code>tl_generate_args</code> is a dictionary of transformer-lens generation arguments. You can specify the model name as anything, it will not affect the model you are using.</p>
<p>Here’s an example:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a HookedTransformer model and set up all the hooks</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>tl_model <span class="op">=</span> HookedTransformer(...)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model args with the TransformerLens model and generation parameters</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>model_args <span class="op">=</span> {</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tl_model"</span>: tl_model,</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tl_generate_args"</span>: {</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"max_new_tokens"</span>: <span class="dv">50</span>,</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"temperature"</span>: <span class="fl">0.7</span>,</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"do_sample"</span>: <span class="va">True</span>,</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Use with get_model()</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(<span class="st">"transformer_lens/your-model-name"</span>, <span class="op">**</span>model_args)</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Or use directly in eval()</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a><span class="bu">eval</span>(<span class="st">"arc.py"</span>, model<span class="op">=</span><span class="st">"transformer_lens/your-model-name"</span>, model_args<span class="op">=</span>model_args)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<ol type="1">
<li>Please note that tool calling is not yet supported for TransformerLens models.</li>
<li>Since the model is loaded dynamically, it is not possible to use cli arguments to specify the model.</li>
</ol>
</section>
</section>
<section id="ollama" class="level2">
<h2 class="anchored" data-anchor-id="ollama">Ollama</h2>
<p>To use the <a href="https://ollama.com/">Ollama</a> provider, install the <code>openai</code> package (which Ollama provides a compatible backend for) and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> ollama/llama3.1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that you should be sure that Ollama is running on your system before using it with Inspect.</p>
<p>You can enable <a href="#tool-emulation-openai">Tool Emulation</a> for Ollama models using the <code>emulate_tools</code> custom model arg (<code>-M</code>).</p>
<p>The following environment variables are supported by the Ollma provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>OLLAMA_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>http://localhost:11434/v1</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="llama-cpp-python" class="level2">
<h2 class="anchored" data-anchor-id="llama-cpp-python">Llama-cpp-python</h2>
<p>To use the <a href="https://llama-cpp-python.readthedocs.io/en/latest/">Llama-cpp-python</a> provider, install the <code>openai</code> package (which llama-cpp-python provides a compatible backend for) and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> llama-cpp-python/llama3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that you should be sure that the <a href="https://llama-cpp-python.readthedocs.io/en/latest/server/">llama-cpp-python server</a> is running on your system before using it with Inspect.</p>
<p>The following environment variables are supported by the llama-cpp-python provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>LLAMA_CPP_PYTHON_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>http://localhost:8000/v1</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="openai-api" class="level2">
<h2 class="anchored" data-anchor-id="openai-api">OpenAI Compatible</h2>
<p>If your model provider makes an OpenAI API compatible endpoint available, you can use it with Inspect via the <code>openai-api</code> provider, which uses the following model naming convention:</p>
<pre><code>openai-api/&lt;provider-name&gt;/&lt;model-name&gt;</code></pre>
<p>Inspect will read environment variables corresponding to the api key and base url of your provider using the following convention (note that the provider name is capitalized):</p>
<pre><code>&lt;PROVIDER_NAME&gt;_API_KEY
&lt;PROVIDER_NAME&gt;_BASE_URL</code></pre>
<p>Note that hyphens within provider names will be converted to underscores so they conform to requirements of environment variable names. For example, if the provider is named <code>awesome-models</code> then the API key environment variable should be <code>AWESOME_MODELS_API_KEY</code>.</p>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Here is how you would access DeepSeek using the <code>openai-api</code> provider:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DEEPSEEK_API_KEY</span><span class="op">=</span>your-deepseek-api-key</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DEEPSEEK_BASE_URL</span><span class="op">=</span>https://api.deepseek.com</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> openai-api/deepseek/deepseek-reasoner</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="responses-api-1" class="level3">
<h3 class="anchored" data-anchor-id="responses-api-1">Responses API</h3>
<p>You can enable the use of the Responses API with the <code>openai-api</code> provider by passing the <code>responses_api</code> model arg. For example:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> inspect eval arc.py <span class="at">--model</span> openai-api/<span class="op">&lt;</span>provider<span class="op">&gt;</span>/<span class="op">&lt;</span>model<span class="op">&gt;</span> -M responses_api=true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or using the <span class="element-type-name ref-interlink"><a href="./reference/inspect_ai.html#eval">eval()</a></span> function:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="bu">eval</span>(<span class="st">"arc.py"</span>, model<span class="op">=</span><span class="st">"openai-api/&lt;provider&gt;/&lt;model&gt;"</span>, model_args<span class="op">=</span><span class="bu">dict</span>(responses_api<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="tool-emulation-openai" class="level3">
<h3 class="anchored" data-anchor-id="tool-emulation-openai">Tool Emulation</h3>
<p>When using OpenAI compatible model providers, tool calling support can be ‘emulated’ for models that don’t yet support it. Use the <code>emulate_tools</code> model arg to force tool emulation:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval ctf.py <span class="at">--model</span> openai-api/<span class="op">&lt;</span>provider<span class="op">&gt;</span>/<span class="op">&lt;</span>model<span class="op">&gt;</span> -M emulate_tools=true</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Tool calling emulation works by encoding tool JSON schema in an XML tag and asking the model to make tool calls using another XML tag. This works with varying degrees of efficacy depending on the model and the complexity of the tool schema. Before using tool emulation you should always check if your provider implements native support for tool calling on the model you are using, as that will generally work better.</p>
</section>
</section>
<section id="openrouter" class="level2">
<h2 class="anchored" data-anchor-id="openrouter">OpenRouter</h2>
<p>To use the <a href="https://openrouter.ai/">OpenRouter</a> provider, install the <code>openai</code> package (which the OpenRouter service provides a compatible backend for), set your credentials, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">OPENROUTER_API_KEY</span><span class="op">=</span>your-openrouter-api-key</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> openrouter/gryphe/mythomax-l2-13b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the <code>openrouter</code> provider, the following custom model args (<code>-M</code>) are supported (click the argument name to see its docs on the OpenRouter site):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 81%">
</colgroup>
<thead>
<tr class="header">
<th>Argument</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://openrouter.ai/docs/features/model-routing#the-models-parameter"><code>models</code></a></td>
<td><code>-M "models=anthropic/claude-3.5-sonnet, gryphe/mythomax-l2-13b"</code></td>
</tr>
<tr class="even">
<td><a href="https://openrouter.ai/docs/features/provider-routing"><code>provider</code></a></td>
<td><code>-M "provider={ 'quantizations': ['int8'] }"</code></td>
</tr>
<tr class="odd">
<td><a href="https://openrouter.ai/docs/features/message-transforms"><code>transforms</code></a></td>
<td><code>-M "transforms=['middle-out']"</code></td>
</tr>
</tbody>
</table>
<p>In addition, <a href="#tool-emulation-openai">Tool Emulation</a> is available for models that don’t yet support tool calling in their API.</p>
<p>The following environment variables are supported by the OpenRouter AI provider</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>OPENROUTER_API_KEY</code></td>
<td>API key credentials (required).</td>
</tr>
<tr class="even">
<td><code>OPENROUTER_BASE_URL</code></td>
<td>Base URL for requests (optional, defaults to <code>https://openrouter.ai/api/v1</code>)</td>
</tr>
</tbody>
</table>
</section>
<section id="hugging-face-inference-providers" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face-inference-providers">Hugging Face Inference Providers</h2>
<p>To use <a href="https://huggingface.co/docs/inference-providers">Hugging Face Inference Providers</a>, install the <code>openai</code> package (which provides the compatibility layer), set your Hugging Face token, and specify a model using the <code>--model</code> option:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HF_TOKEN</span><span class="op">=</span>your-huggingface-token</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> hf-inference-providers/openai/gpt-oss-120b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The above will automatically select the provider for you. If you want to use a specific provider you can append <code>:</code> followed by the provider name. To use cerebras for example, you would do the following:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HF_TOKEN</span><span class="op">=</span>your-huggingface-token</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="ex">inspect</span> eval arc.py <span class="at">--model</span> hf-inference-providers/openai/gpt-oss-120b:cerebras</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>HF Inference Providers provides unified access to hundreds of machine learning models through multiple world-class inference providers (Cerebras, Groq, Together AI, etc.) with automatic provider routing and failover.</p>
<p>The following environment variables are supported by the HF Inference Providers:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>HF_TOKEN</code></td>
<td>Hugging Face token with appropriate permissions (required).</td>
</tr>
</tbody>
</table>
</section>
<section id="custom-models" class="level2">
<h2 class="anchored" data-anchor-id="custom-models">Custom Models</h2>
<p>If you want to support another model hosting service or local model source, you can add a custom model API. See the documentation on <a href="./extensions.html#sec-model-api-extensions">Model API Extensions</a> for additional details.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/inspect\.aisi\.org\.uk\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            trigger: 'click',
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            positionFixed: true,
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./models.html" class="pagination-link" aria-label="Using Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Using Models</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./caching.html" class="pagination-link" aria-label="Caching">
        <span class="nav-page-text">Caching</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://aisi.gov.uk/">
<p>UK AI Security Institute</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/UKGovernmentBEIS/inspect_ai">
<p>Code</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/UKGovernmentBEIS/inspect_ai/blob/main/CHANGELOG.md">
<p>Changelog</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/UKGovernmentBEIS/inspect_ai/blob/main/LICENSE">
<p>License</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/UKGovernmentBEIS/inspect_ai/issues">
<p>Issues</p>
</a>
  </li>  
</ul>
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/UKGovernmentBEIS/inspect_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/AISecurityInst">
      <i class="bi bi-twitter" role="img" aria-label="UK AI Security Institute Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/UKGovernmentBEIS/inspect_ai/">
      <i class="bi bi-github" role="img" aria-label="Inspect on GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>